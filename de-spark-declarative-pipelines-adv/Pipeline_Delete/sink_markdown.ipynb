{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9179fb9-7c87-4217-82aa-ba57cf7be2b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## G. Delta Table Sinks with Unity Catalog Integration\n",
    "\n",
    "In this step you will create Delta table sinks from your silver data so external Iceberg readers can access the results. This gives downstream tools a stable, governed table to read from while still keeping your main processing inside the pipeline.\n",
    "\n",
    "**Why would you do this?**\n",
    "- Delta sinks persist your cleaned silver data as Unity Catalog tables.\n",
    "- You can enable Iceberg compatibility on these tables so external systems can read them.\n",
    "- You can currently sink your pipeline to:\n",
    "  - Delta tables in Unity Catalog (managed or external)\n",
    "  - Apache Kafka\n",
    "  - Azure Event Hubs\n",
    "  - Custom Python sinks (any system you connect to)\n",
    "\n",
    "**NOTES:** \n",
    "- For more details, see the Databricks documentation on [using sinks with Lakeflow Spark Declarative Pipelines](https://docs.databricks.com/aws/en/ldp/sinks).\n",
    "- **Only the Python API is available for creating sinks.**\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sink_markdown",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
