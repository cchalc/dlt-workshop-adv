{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6b7244b-0308-423f-848b-e1972d1a762a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# INCLUDE_HEADER_TRUE\n",
    "# INCLUDE_FOOTER_TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67fb5114-a3c8-4b9c-ad79-5f0f11bd55f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo - Multi Flow Data Pipeline with Liquid Clustering and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df443581-5789-4f79-a6e3-bec4198cb234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This demonstration showcases how to build a robust incremental data pipeline using Lakeflow Spark Declarative Pipelines (SDP) to consolidate data from multiple subsidiaries (data sources) into a single target streaming table. \n",
    "\n",
    "You'll work with three fictional company subsidiaries: Bright Home, Lumina Sports, and Northstar Outfitters, each producing transaction data in different formats (`CSV` and `JSON`). The demo illustrates how to overcome common pipeline challenges including multiple flows into a single table, schema mismatches, data quality issues, and performance optimization requirements.\n",
    "\n",
    "Through hands-on implementation, you'll create a complete medallion architecture pipeline that incrementally ingests multiple data sources into a single bronze table using flows, applies data quality constraints and transformations in the silver layer with liquid clustering optimization, and creates business intelligence materialized views in the gold layer. \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this demonstration, you will be able to:\n",
    "- **Ingest multiple data sources into one bronze table** using Spark Declarative Pipelines having different file formats like CSV and JSON.\n",
    "\n",
    "- **Standardize schemas in the bronze layer** and map correct data types in silver layer to resolve differences across source systems.\n",
    "\n",
    "- **Add data quality checks and enable liquid clustering** in the silver tables to enforce basic rules and improve query performance.\n",
    "\n",
    "- **Build incremental materialized views** in the gold layer that refresh automatically and provide ready-to-use analytics.\n",
    "\n",
    "- **Run and monitor the full pipeline run** across bronze, silver, and gold, including incremental loads and data lineage tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e6c8cb-93c6-4d69-a974-ba9db9d29af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Multi Flow Pipeline Demonstration Overview\n",
    "In this demonstration, you'll build a Lakeflow Spark Declarative Pipeline that performs the full medallion flow from raw ingestion of multiple raw data sources to curated analytics\n",
    "\n",
    "1. **Use multiple flows (3)** to incrementally ingest files from three cloud storage locations and write into a single bronze table. \n",
    "    - Each volume is a daily orders drop for a specific subsidiary that the company owns. We want all this data ingested into a single table for overall analysis.\n",
    "2. **Define and build the silver table** with a clean schema, apply basic data quality constraints, and **enable liquid clustering** for query performance as the data continues to grow.\n",
    "3. **Create gold materialized views** that automatically refresh and provide ready-to-use analytics.\n",
    "\n",
    "![Multi Flow Pipeline Overview](./Includes/images/multi_flow/multi_flow_demo_pipeline_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "652dcfc4-ecd3-4459-b2c4-bfbf37ab16c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT SERVERLESS COMPUTE VERSION 4\n",
    "\n",
    "This demonstration was developed using **Serverless v4**. \n",
    "- Selecting a serverless version: [Select an environment version](https://docs.databricks.com/aws/en/compute/serverless/dependencies#-select-an-environment-version)\n",
    "\n",
    "**NOTE:** While all purpose compute or other **Serverless** versions may work, they were not tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d137f339-9e59-4c1e-a15d-7c0c0b58494b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "Follow the cells below to set up your Workspace for the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6050910-4a99-484b-add1-05caad6b6bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A1. Access Marketplace Data\n",
    "\n",
    "##### NOTE: If you are running this lab in **Vocareum** as a **Partner**, the share is already installed and available as **dbacademy_retail**. Please use this as the value for the `your_marketplace_share_catalog_name` variable below.\n",
    "\n",
    "If you are running this in your own Workspace, complete the following steps to get your own copy of the Marketplace data:\n",
    "\n",
    "1. Open **Databricks Marketplace** in a new tab.  \n",
    "\n",
    "2. Search for `Simulated Retail Customer Data`.  \n",
    "\n",
    "3. Select the tile titled **Simulated Retail Customer Data (Databricks provided)**.  \n",
    "\n",
    "4. Click **Get instant access**.  \n",
    "\n",
    "5. **Enter a unique catalog name** for your share to avoid receiving a duplicate catalog error in shared Workspaces. For example: `dbacademy_retail_yourname`.  \n",
    "\n",
    "6. Review and accept the terms, then click **Get instant access** to complete the setup.\n",
    "\n",
    "7. Update the variable `your_marketplace_share_catalog_name` in cell below to point to your shared catalog from Marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e74a254d-b76b-447d-a778-dba57374da6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Update the variable below to reference your marketplace catalog name\n",
    "\n",
    "## NOTE: If you are using Vocareum use the value 'dbacademy_retail' catalog below\n",
    "your_marketplace_share_catalog_name = 'YOUR_MARKETPLACE_SHARE_CATALOG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10e15aa3-ca92-4659-8a1a-4ab79f844695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A2. Configure Your Catalog and Schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb03fae3-732d-4862-8c3c-0887f282b261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. In this step, you must **specify a catalog that you own or have write access to**.\n",
    "    - If you already have a catalog, update the code below to use its name.\n",
    "    - If you do not yet have one, create a catalog first, then return and update the code.\n",
    "\n",
    "    Replace the value for the `my_catalog` variable with your catalog name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c411c3a2-e065-4a2c-a509-fad12d71a9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_catalog = 'YOUR_CATALOG'   ## <---- Replace with your catalog name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a01fd74e-9580-4ae8-b474-25185b0fc1d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cells below to setup your demonstration environment. \n",
    "\n",
    "**NOTES: The setup will:**\n",
    "\n",
    "- Check the variables from above were created\n",
    "- Create three schemas in your specified catalog:  \n",
    "    - **multi_flow_1_bronze**\n",
    "    - **multi_flow_2_silver**\n",
    "    - **multi_flow_3_gold**  \n",
    "- Creates a three volumes in your **your-catalog.multi_flow_1_bronze** schema and adds a single JSON file in each volume.\n",
    "- Checks your specified compute\n",
    "\n",
    "This ensures that all schemas, tables and objects are created in your catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6ba51c-da5a-49b6-aa83-84a6b53f8a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-multiple-flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb4084b-7199-4b7f-ba4a-1e2a8670e619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_vol_path = multi_flow_demo_setup(\n",
    "    my_catalog = my_catalog, \n",
    "    marketplace_catalog = your_marketplace_share_catalog_name, \n",
    "    schema = 'multi_flows',\n",
    "    source_volumes = ['bright_home_orders','lumina_sports_orders','northstar_outfitters_orders'],\n",
    "    reset_volume = False  ## <-- Set to True to delete all files in your volumes to start fresh if you've already complete the demo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79fe67c7-b132-48b6-a711-d0cd467b88cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the cell below to view the value of the `my_vol_path` variable.\n",
    "\n",
    "   Confirm that the value references your **your-catalog.multi_flows_1_bronze** path. This will be used to dynamically reference your source volumes throughout this demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745c0d3f-a31c-4ba4-be52-486a035e9ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(my_vol_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08ecc754-ca3e-4374-9a6e-ac112051cf99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore the Source Volumes for Ingesting Multiple Flows into a Single Target\n",
    "\n",
    "Before building multiple flows that write to a single target streaming table, start by exploring the raw source data stored in in the three volumes.  \n",
    "\n",
    "Each **volume** represents a **separate sales system** for a different subsidiary within our fictional company:\n",
    "\n",
    "- **B1.** Bright Home Orders volume (`CSV` files)  \n",
    "- **B2.** Lumina Sports Orders volume (`CSV` files)  \n",
    "- **B3.** Northstar Outfitters Orders volume (`JSON` files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea0f85f-fdba-4af0-b948-f2198f10425c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B1. Bright Home Orders Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32139c0b-9c7f-42a5-b3f5-6990833e22ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. View the files in the **multi_flows_1_bronze.bright_home_orders** volume.\n",
    "\n",
    "   Notice that only one `CSV` file currently exists in this volume for the sales on **2025-11-01**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b0fa5d-b898-4448-b96c-b97cf06bfd4a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":634},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763298434421}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"LIST '{my_vol_path}/bright_home_orders'\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3d117de-2f79-4c7a-ae15-cfaab21b1d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Explore the raw data for **bright_home_orders**. The cell below performs the following:\n",
    "\n",
    "   a. Counts the number of records in the file within the volume.  \n",
    "   b. Describes the default ingestion data types for each column of the `CSV` file.  \n",
    "   c. Previews the data.  \n",
    "\n",
    "In the output, notice the following:\n",
    "- **157** rows are present in this file.  \n",
    "- The schema is inferred and returns a variety of data types.  \n",
    "- This is simple sales order data from the company's **Bright Home** subsidiary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac300fa5-f6b7-424f-99d2-69c78267865a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a. Row count\n",
    "df_count = spark.sql(f\"\"\"\n",
    "    SELECT count(*) AS TotalRows\n",
    "    FROM read_files('{my_vol_path}/bright_home_orders')\n",
    "\"\"\")\n",
    "display(df_count)\n",
    "\n",
    "# b. Schema\n",
    "df_schema = spark.sql(f\"\"\"\n",
    "    DESCRIBE SELECT * \n",
    "    FROM read_files('{my_vol_path}/bright_home_orders')\n",
    "\"\"\")\n",
    "display(df_schema)\n",
    "\n",
    "# c. Preview rows\n",
    "df_preview = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_files('{my_vol_path}/bright_home_orders')\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "display(df_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66457677-a3ff-4e07-92b5-5863db10c725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Lumina Sports Orders Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66606795-45d0-4778-a0b0-4f52cdc7d617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. View the files in the **multi_flows_1_bronze.lumina_sports_orders** volume.\n",
    "\n",
    "   Notice that only one `CSV` file currently exists in this volume for the sales on **2025-11-01**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d058e07a-217d-40a6-9ba2-dc157dec698b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"LIST '{my_vol_path}/lumina_sports_orders'\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf4fc88-02ec-4290-b86f-e0f6cb3862a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Explore the raw data for **Lumina Sports**. The cell below performs the following:\n",
    "\n",
    "   a. Counts the number of records in the file within the volume.  \n",
    "   b. Describes the default ingestion data types for each column of the `CSV` file.  \n",
    "   c. Previews the data.  \n",
    "\n",
    "In the output, notice the following:\n",
    "- **110** rows are present in this file.  \n",
    "- The schema is inferred and returns a variety of data types.  \n",
    "- This is simple sales order data from the company's **Lumina Sports** subsidiary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba342e9-208b-4b76-8470-de36278b2df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a. Row count\n",
    "df_count = spark.sql(f\"\"\"\n",
    "    SELECT count(*) AS TotalRows\n",
    "    FROM read_files('{my_vol_path}/lumina_sports_orders')\n",
    "\"\"\")\n",
    "display(df_count)\n",
    "\n",
    "# b. Schema\n",
    "df_schema = spark.sql(f\"\"\"\n",
    "    DESCRIBE SELECT *\n",
    "    FROM read_files('{my_vol_path}/lumina_sports_orders')\n",
    "\"\"\")\n",
    "display(df_schema)\n",
    "\n",
    "# c. Full preview\n",
    "df_preview = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_files('{my_vol_path}/lumina_sports_orders')\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "display(df_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b5f3781-a54b-4445-be33-eefab3dc2376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. Northstar Outfitters Orders Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0ff0a27-70e0-47d3-9902-d8db78812a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. View the files in the **multi_flows_1_bronze.northstar_outfitters_orders** volume.\n",
    "\n",
    "   Notice that only one `JSON` file currently exists in this volume for the sales on **2025-11-01**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7cba1c-ca09-49e0-8bcd-e806700be185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"LIST '{my_vol_path}/northstar_outfitters_orders'\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee9a1039-fee4-4bf5-9698-6308887d0fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Explore the raw data for **Northstar Outfitters**. The cell below performs the following:\n",
    "\n",
    "   a. Counts the number of records in the file within the volume.  \n",
    "   b. Describes the default ingestion data types for each column of the `JSON` file.  \n",
    "   c. Previews the data.  \n",
    "\n",
    "In the output, notice the following:\n",
    "- **182** rows are present in this file.  \n",
    "- The schema is inferred and returns a variety of data types.  \n",
    "- This is simple sales order data from the company's **Northstar Outfitters** subsidiary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97d8bd7-79a9-4f17-9932-ca5ca1643f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a. Row count\n",
    "df_count = spark.sql(f\"\"\"\n",
    "    SELECT count(*) AS TotalRows\n",
    "    FROM read_files('{my_vol_path}/northstar_outfitters_orders')\n",
    "\"\"\")\n",
    "display(df_count)\n",
    "\n",
    "# b. Schema\n",
    "df_schema = spark.sql(f\"\"\"\n",
    "    DESCRIBE SELECT *\n",
    "    FROM read_files('{my_vol_path}/northstar_outfitters_orders')\n",
    "\"\"\")\n",
    "display(df_schema)\n",
    "\n",
    "# c. Full preview\n",
    "df_preview = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_files('{my_vol_path}/northstar_outfitters_orders')\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "display(df_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04302b71-fe55-43de-95cc-49f5bd304808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B4. Raw Data Exploration Summary\n",
    "\n",
    "Below is a quick overview of what we discovered after examining the raw source volumes. \n",
    "\n",
    "#### Raw Cloud Storage Overview\n",
    "Each source volume contains one file with a small number of sales for our demonstration.\n",
    "\n",
    "| Source Data (volume)            | File Format | # of Rows | # of Files | Sales Date |\n",
    "|---------------------------------|-------------|-----------|------------|------------|\n",
    "| Bright Home Orders              | CSV         | 157       | 1          | 2025-11-01|\n",
    "| Lumina Sports Orders            | CSV         | 110       | 1          | 2025-11-01|\n",
    "| Northstar Outfitters Orders     | JSON        | 182       | 1          | 2025-11-01|\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Schema Differences Comparison and Issues\n",
    "\n",
    "- Each raw data source uses its own structure (`CSV` or `JSON`).  \n",
    "- When ingesting `CSV`, the inferred column types often differ from `JSON`.  \n",
    "- Since each format infers schemas independently, combining these three flows into a single target table will introduce **schema mismatches that lead to ingestion conflicts**.\n",
    "\n",
    "| **Column Name**     | **bright_home_orders (CSV)** | **lumina_sports_orders (CSV)** | **northstar_outfitters_orders (JSON)** |\n",
    "|---------------------|------------------------|---------------------------|---------------------------------|\n",
    "| subsidiary_id       | string                 | string                    | string                          |\n",
    "| order_id            | string                 | string                    | string                          |\n",
    "| **order_timestamp** | **timestamp**          | **timestamp**             | **string**                      |\n",
    "| customer_id         | string                 | string                    | string                          |\n",
    "| region              | string                 | string                    | string                          |\n",
    "| country             | string                 | string                    | string                          |\n",
    "| city                | string                 | string                    | string                          |\n",
    "| channel             | string                 | string                    | string                          |\n",
    "| sku                 | string                 | string                    | string                          |\n",
    "| category            | string                 | string                    | string                          |\n",
    "| **qty**             | **int**                | **int**                   | **bigint**                      |\n",
    "| unit_price          | double                 | double                    | double                          |\n",
    "| **discount_pct**    | **int**                | **int**                   | **bigint**                      |\n",
    "| coupon_code         | string                 | string                    | string                          |\n",
    "| total_amount        | double                 | double                    | double                          |\n",
    "| **order_date**      | **date**               | **date**                  | **string**                      |\n",
    "| _rescued_data       | string                 | string                    | string                          |\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Goal: Ingest All Raw Source Files Into One Bronze Streaming Table\n",
    "\n",
    "To successfully combine all three sources into a single bronze streaming table, we will need to standardize the schema. \n",
    "\n",
    "To do this, we will **ingest every column as a `STRING`** into the bronze table.\n",
    "\n",
    "This avoids data type conflicts between `CSV` and `JSON` files, since each format infers types differently. Normalizing everything to `STRING` keeps the bronze layer predictable, prevents ingestion failures, and lets us apply the correct data types later in the silver layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c714cae7-cf87-4445-9d0d-084f7bebf1aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Create the Spark Declarative Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b31f42b1-e9c8-4d9b-a680-b5adc3475b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Enable the Lakeflow Pipelines Editor\n",
    "\n",
    "Complete the following steps to confirm or enable the **Lakeflow Pipelines Editor**:\n",
    "\n",
    "1. In the top-right corner of the workspace, select your **account icon** ![Account Icon](./Includes/images/account_icon.png) (*Your icon letter will differ*).  \n",
    "\n",
    "2. Right-click **Settings** and choose **Open link in new tab**.  \n",
    "\n",
    "3. In the left sidebar, select **Developer** under **User**.  \n",
    "\n",
    "4. In the **Experimental features** section, locate **Lakeflow Pipelines Editor** and toggle it **on**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f29395-9236-4345-b94c-c4cbca16483b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Create a Lakeflow Spark Declarative Pipeline using the Lakeflow Pipelines Editor\n",
    "Complete the following steps to create your Spark Declarative Pipeline:\n",
    "\n",
    "1. In the main navigation pane, right-click **Jobs & Pipelines** and select **Open link in New Tab**.  \n",
    "\n",
    "2. In the new tab, select **Create â†’ ETL Pipeline**.  \n",
    "\n",
    "   **NOTE:** If prompted to **Try the new Lakeflow Pipelines Editor**, choose **Enable Lakeflow Pipelines Editor**. This appears only if you did not complete the previous step.  \n",
    "\n",
    "3. At the top, complete the following:\n",
    "   - Name your pipeline `demo_multi_flow_yourname`\n",
    "   - Select your default **catalog** and **schema**:  \n",
    "        - **Catalog:** The catalog you specified for this notebook  \n",
    "        - **Schema:** **multi_flows_1_bronze**  \n",
    "\n",
    "4. Select **Start with an empty file**. In the pop-up window, specify the following:  \n",
    "   - For **Location where the pipeline folder will be created** - Specify the folder this notebook resides in.\n",
    "   - Select **SQL**\n",
    "   - Select **Create**\n",
    "\n",
    "5. Rename the **transformations** folder to `ingest_multiple_flows`.\n",
    "\n",
    "6. Rename the **my_transformations.sql** file to `flow_ingestion.sql`.\n",
    "\n",
    "7. Leave the **Lakeflow Pipelines Editor** page open.\n",
    "\n",
    "#### Checkpoint\n",
    "![Create SDP Checkpoint](./Includes/images/multi_flow/pipeline_creation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95194459-b910-4008-883f-f038fe0ee5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Using Multiple Flows to Write to a Single Target\n",
    "\n",
    "In many enterprise environments, data arrives from several systems that must be consolidated into a single table for downstream processing.\n",
    "\n",
    "In this example, the company has **three subsidiaries, each producing transaction data in slightly different raw file formats**. \n",
    "\n",
    "With Spark Declarative Pipelines, you can define multiple flows that write to the same target streaming table, allowing all raw files in cloud storage to be **incrementally ingested into one unified destination**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac2c06d-1af9-43db-8dd5-2505494f5511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. Create the Bronze Target Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c551da52-0b39-4e49-a59d-9371d9f43448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Start by creating the **bronze streaming table** that will serve as the landing zone for all incoming transaction data. \n",
    "\n",
    "    This table ingests every column as `STRING` to ensure compatibility across the different source systems.\n",
    "\n",
    "\n",
    "    Copy the SQL code below and paste it into your `flow_ingestion.sql` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "146d93d9-a94f-4c6f-9131-7a848ad25853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "------------------------------------------\n",
    "-- CREATE THE BRONZE TABLE STRUCTURE\n",
    "------------------------------------------\n",
    "CREATE OR REPLACE STREAMING TABLE multi_flows_1_bronze.orders_bronze_flows_demo\n",
    "(\n",
    "  subsidiary_id   STRING,\n",
    "  order_id        STRING,\n",
    "  order_timestamp STRING,\n",
    "  customer_id     STRING,\n",
    "  region          STRING,\n",
    "  country         STRING,\n",
    "  city            STRING,\n",
    "  channel         STRING,\n",
    "  sku             STRING,\n",
    "  category        STRING,\n",
    "  qty             STRING,\n",
    "  unit_price      STRING,\n",
    "  discount_pct    STRING,\n",
    "  coupon_code     STRING,\n",
    "  total_amount    STRING,\n",
    "  order_date      STRING,\n",
    "  source_file     STRING,   -- Added by the _metadata column to return the source file name\n",
    "  file_mod_time   TIMESTAMP -- Added by the _metadata column to return file modification time of the file. Returns a consistent value\n",
    ")\n",
    "COMMENT \"Creates a single bronze streaming table with orders from all subsidiaries using multiple flows.\"\n",
    "TBLPROPERTIES (\n",
    "  'pipelines.reset.allowed' = false    -- prevent full table refreshes on the bronze table\n",
    ");\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff94202-8040-4d5b-9983-0da8064fb304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code**\n",
    "- The `COMMENT` clause adds descriptive metadata to the table for documentation purposes.\n",
    "\n",
    "- The `TBLPROPERTIES` statement configures the following setting:\n",
    "  - **Reset Protection**: The `'pipelines.reset.allowed' = false` property prevents full refreshes on the streaming table, which helps avoid accidentally removing checkpoints and truncating the streaming table data.\n",
    "\n",
    "#### IMPORTANT: Understanding Full Table Refresh Protection\n",
    "\n",
    "This protection is particularly important when your raw data source automatically removes files after a certain timeframe. Without this setting, data that is no longer present in the source directory would not be reingested into the target table during a **Run pipeline with full table refresh** operation.\n",
    "\n",
    "**NOTE:** For guidance on when to use full refreshes, see the [Should I use a full refresh?](https://docs.databricks.com/aws/en/ldp/updates#should-i-use-a-full-refresh) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae90a9a3-512a-48f7-a2fc-b64e75aed586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Configure the Pipeline Parameters\n",
    "\n",
    "1. Run the cell below to retrieve the key value pairs needed to set your pipeline configuration parameters for each **raw data source volume**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf3570d-71b6-4722-bf1a-5dfde9417773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config_parameters = [\n",
    "    ('bright_home_orders_source',        f'{my_vol_path}/bright_home_orders'),\n",
    "    ('lumina_sports_orders_source',      f'{my_vol_path}/lumina_sports_orders'),\n",
    "    ('northstar_outfitters_orders_source', f'{my_vol_path}/northstar_outfitters_orders')\n",
    "]\n",
    "\n",
    "for key, value in config_parameters:\n",
    "    print(f\"Key: {key}\\nValue: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d5b99e5-cd0a-4902-b02b-f0ea71403669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Copy the paths above and add each one by one as a configuration parameter in your **Spark Declarative Pipeline**.\n",
    "\n",
    "   This will allow your pipeline to reference each volume through parameters.\n",
    "\n",
    "   a. Select **Settings** in your pipeline tab.  \n",
    "\n",
    "   b. Under **Configuration**, select **Add configuration**. \n",
    "\n",
    "   c. For each **Key**, enter the key name shown above.  \n",
    "\n",
    "   d. For each **Value**, enter the corresponding volume path.  \n",
    "\n",
    "   e. Select **Save**.\n",
    "\n",
    "   **NOTE:** For more details on configuration parameters, see the Databricks documentation: [Use parameters with Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/ldp/parameters)\n",
    "\n",
    "\n",
    "#### Checkpoint (your path will vary)\n",
    "<img src=\"./Includes/images/multi_flow/checkpoint_config_params.png\" alt=\"Config Parameter Checkpoint\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b6b3895-685f-448a-83bc-4af7ae5d2b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D3. Configure Flow from the Store Bright Home Orders Volume\n",
    "\n",
    "1. Copy the code below and paste into your `flow_ingestion.sql` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "e1faf9ae-ecac-489a-a39f-50248c16da96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "------------------------------------------\n",
    "-- BRONZE FLOW - BRIGHT HOME\n",
    "------------------------------------------\n",
    "-- Read CSV files from the bright_home_orders volume\n",
    "CREATE FLOW bright_home_orders_flow\n",
    "AS INSERT INTO multi_flows_1_bronze.orders_bronze_flows_demo BY NAME\n",
    "SELECT\n",
    "  CAST(subsidiary_id AS STRING) AS subsidiary_id,\n",
    "  CAST(order_id AS STRING) AS order_id,\n",
    "  CAST(order_timestamp AS STRING) AS order_timestamp,\n",
    "  CAST(customer_id AS STRING) AS customer_id,\n",
    "  CAST(region AS STRING) AS region,\n",
    "  CAST(country AS STRING) AS country,\n",
    "  CAST(city AS STRING) AS city,\n",
    "  CAST(channel AS STRING) AS channel,\n",
    "  CAST(sku AS STRING) AS sku,\n",
    "  CAST(category AS STRING) AS category,\n",
    "  CAST(qty AS STRING) AS qty,\n",
    "  CAST(unit_price AS STRING) AS unit_price,\n",
    "  CAST(discount_pct AS STRING) AS discount_pct,\n",
    "  CAST(coupon_code AS STRING) AS coupon_code,\n",
    "  CAST(total_amount AS STRING) AS total_amount,\n",
    "  CAST(order_date AS STRING) AS order_date,\n",
    "  _metadata.file_name AS source_file,\n",
    "  _metadata.file_modification_time AS file_mod_time\n",
    "FROM STREAM read_files(\n",
    "    '${bright_home_orders_source}',   -- Uses the configuration parameter to point to the bright_home_orders volume\n",
    "    format => 'csv',\n",
    "    header => true\n",
    ");\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "595dcaa9-33af-4271-a081-3f142da76230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code**\n",
    "- All business columns are cast to `STRING` so this flow aligns with the unified bronze schema, while the **metadata** columns keep their native types.  \n",
    "- The **metadata columns** capture the source **file name and modification time**, which helps with lineage and debugging.  \n",
    "- The `FROM STREAM read_files('${bright_home_orders_source}', ...)` clause uses your configuration parameter to reference the volume path and relies on Auto Loader for incremental ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "218d5534-67e4-48a4-8df8-b800afc74634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D4. Configure Flow from Store Lumina Sports Orders Volume\n",
    "\n",
    "1. Copy the code below and paste into your `flow_ingestion.sql` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "dd9308d7-44ea-405f-bb20-5c6aab6384cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "------------------------------------------\n",
    "-- BRONZE FLOW - LUMINA SPORTS\n",
    "------------------------------------------\n",
    "-- Read CSV files from the lumina_sports_orders volume\n",
    "CREATE FLOW lumina_sports_orders_flow\n",
    "AS INSERT INTO multi_flows_1_bronze.orders_bronze_flows_demo BY NAME\n",
    "SELECT\n",
    "  CAST(subsidiary_id AS STRING) AS subsidiary_id,\n",
    "  CAST(order_id AS STRING) AS order_id,\n",
    "  CAST(order_timestamp AS STRING) AS order_timestamp,\n",
    "  CAST(customer_id AS STRING) AS customer_id,\n",
    "  CAST(region AS STRING) AS region,\n",
    "  CAST(country AS STRING) AS country,\n",
    "  CAST(city AS STRING) AS city,\n",
    "  CAST(channel AS STRING) AS channel,\n",
    "  CAST(sku AS STRING) AS sku,\n",
    "  CAST(category AS STRING) AS category,\n",
    "  CAST(qty AS STRING) AS qty,\n",
    "  CAST(unit_price AS STRING) AS unit_price,\n",
    "  CAST(discount_pct AS STRING) AS discount_pct,\n",
    "  CAST(coupon_code AS STRING) AS coupon_code,\n",
    "  CAST(total_amount AS STRING) AS total_amount,\n",
    "  CAST(order_date AS STRING) AS order_date,\n",
    "  _metadata.file_name AS source_file,\n",
    "  _metadata.file_modification_time AS file_mod_time\n",
    "FROM STREAM read_files(\n",
    "  '${lumina_sports_orders_source}',   -- Uses the configuration parameter to point to the lumina sports volume\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ");\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeb38b9f-403c-410d-9d95-781b70ab89a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code**\n",
    "- All business columns are cast to `STRING` so this flow aligns with the unified bronze schema, while the **metadata** columns keep their native types.  \n",
    "- The **metadata columns** capture the source **file name and modification time**, which helps with lineage and debugging.  \n",
    "- The `FROM STREAM read_files('${lumina_sports_orders_source}', ...)` clause uses your configuration parameter to reference the volume path and relies on Auto Loader for incremental ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45809767-ba4f-4417-b3c4-3766f15cffac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D5. Configure Flow from Northstar Outfitters Orders Volume\n",
    "\n",
    "1. Copy the code below and paste into your `flow_ingestion.sql` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "3b734bdd-bb16-4649-841a-8cadcda0faa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "------------------------------------------\n",
    "-- BRONZE FLOW - NORTHSTAR OUTFITTERS\n",
    "------------------------------------------\n",
    "-- Read JSON files from the northstar_outfitters_orders volume\n",
    "CREATE FLOW northstar_outfitters_orders_flow\n",
    "AS INSERT INTO multi_flows_1_bronze.orders_bronze_flows_demo BY NAME\n",
    "SELECT\n",
    "  CAST(subsidiary_id AS STRING) AS subsidiary_id,\n",
    "  CAST(order_id AS STRING) AS order_id,\n",
    "  CAST(order_timestamp AS STRING) AS order_timestamp,\n",
    "  CAST(customer_id AS STRING) AS customer_id,\n",
    "  CAST(region AS STRING) AS region,\n",
    "  CAST(country AS STRING) AS country,\n",
    "  CAST(city AS STRING) AS city,\n",
    "  CAST(channel AS STRING) AS channel,\n",
    "  CAST(sku AS STRING) AS sku,\n",
    "  CAST(category AS STRING) AS category,\n",
    "  CAST(qty AS STRING) AS qty,\n",
    "  CAST(unit_price AS STRING) AS unit_price,\n",
    "  CAST(discount_pct AS STRING) AS discount_pct,\n",
    "  CAST(coupon_code AS STRING) AS coupon_code,\n",
    "  CAST(total_amount AS STRING) AS total_amount,\n",
    "  CAST(order_date AS STRING) AS order_date,\n",
    "  _metadata.file_name AS source_file,\n",
    "  _metadata.file_modification_time AS file_mod_time\n",
    "FROM STREAM read_files(\n",
    "  '${northstar_outfitters_orders_source}',  -- Uses the configuration parameter to point to the northstar volume\n",
    "  format => 'json'\n",
    ");\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d457ccc-f262-4657-95d4-ad8f479ff1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code**\n",
    "- All business columns are cast to `STRING` so this flow aligns with the unified bronze schema, while the **metadata** columns keep their native types.  \n",
    "- The **metadata columns** capture the source **file name and modification time**, which helps with lineage and debugging.  \n",
    "- The `FROM STREAM read_files('${northstar_outfitters_orders_source}', ...)` clause uses your configuration parameter to reference the volume path and relies on Auto Loader for incremental ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c56423d-556c-43d5-9812-c2f82d031a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D6. Run and Explore the Pipeline\n",
    "\n",
    "1. Run the Spark Declarative Pipeline and confirm it runs successfully. \n",
    "\n",
    "2. Explore the run in the Lakeflow Pipelines Editor.\n",
    "  - Confirm **449** rows were ingested into the bronze table from the three volumes (**157 + 110 + 182**)\n",
    "  - Preview the data in the editor (Select **orders_bronze_flows_demo** -> **Data** tab). Notice data from each volume was incrementally ingested into the bronze table.\n",
    "\n",
    "> **TROUBLESHOOTING:** If your pipeline does not run successfully, confirm that your volumes were created and that your configuration parameters are set correctly.\n",
    "\n",
    "#### Checkpoint\n",
    "\n",
    "<img src=\"./Includes/images/multi_flow/checkpoint_bronze_flows.png\" alt=\"Bronze Flow\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a4f685-af00-4990-b4ad-86d314053822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D7. Query the Bronze Streaming Table\n",
    "1. The query below groups records by the captured **source_file** column so you can see the number of ingested rows per file.\n",
    "\n",
    "    Review how many files were ingested from each source confirming each cloud storage flow was ingested successfully.\n",
    "\n",
    "#### Checkpoint\n",
    "| Source File                 | Total Ingested by Source  |\n",
    "|-----------------------------|---------------------------|\n",
    "| nso_orders_2025-11-01.json  | 182                       |\n",
    "| bsh_orders_2025-11-01.csv   | 157                       |\n",
    "| lms_orders_2025-11-01.csv   | 110                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea76979b-7e87-414d-9295-5a7d2015c16b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"source_file\":246},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763405904339}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT source_file, count(*) AS `Total Ingested by Source`\n",
    "FROM multi_flows_1_bronze.orders_bronze_flows_demo\n",
    "GROUP BY source_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f16b073c-baf2-4d35-97e3-91ddb633a922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Silver Table Data Quality, Optimization, and Transformation\n",
    "\n",
    "In the silver layer, we refine the **raw bronze table** into a clean and consistent **single source of truth (silver table)**. \n",
    "\n",
    "This is where we standardize fields, enforce data quality, and prepare the dataset for downstream analytics.\n",
    "\n",
    "In this section, you will:\n",
    "\n",
    "- Apply data quality constraints  \n",
    "- Clean and standardize fields  \n",
    "- Enable liquid clustering for on the streaming table for optimized performance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15fb2622-793a-425c-bfd0-d39e50e846e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E1. Create a New SQL File in your Pipeline\n",
    "\n",
    "1. Click the kebab menu and select **New File** in the **ingest_multiple_flows** folder.\n",
    "2. Select the language as **SQL**.\n",
    "3. Name the file `silver_transformation.sql`.\n",
    "4. Keep the dataset type as `None Selected`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd12c5a8-5cc2-409c-be41-2aef50e7d7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E2. Create Silver Transactions Table with Liquid Clustering and Data Quality Rules\n",
    "\n",
    "1. Add the following code to your `silver_transformation.sql` file to create a Silver table with:\n",
    "- a defined schema, \n",
    "- enforce consistent column types through `TRY_CAST` \n",
    "- and enable liquid clustering for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "ff992c22-63fe-4007-a027-d5af23a3627b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "CREATE OR REFRESH STREAMING TABLE multi_flows_2_silver.orders_silver_flows_demo\n",
    "(\n",
    "  -- A: Define a fixed schema to prevent schema evolution.\n",
    "  subsidiary_id   STRING,\n",
    "  order_id        STRING,\n",
    "  order_timestamp TIMESTAMP,\n",
    "  order_date      DATE,\n",
    "  customer_id     STRING,\n",
    "  region          STRING,\n",
    "  country         STRING,\n",
    "  city            STRING,\n",
    "  channel         STRING,\n",
    "  sku             STRING,\n",
    "  category        STRING,\n",
    "  qty             INT,\n",
    "  unit_price      DOUBLE,\n",
    "  discount_pct    DOUBLE,\n",
    "  total_amount    DOUBLE,\n",
    "  coupon_code     STRING,\n",
    "\n",
    "  -- B: Data quality constraints to drop or flag or drop invalid rows.\n",
    "  CONSTRAINT qty_valid          EXPECT (qty >= 0) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT total_amount_valid EXPECT (total_amount >= 0) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT timestamp_not_null EXPECT (order_timestamp IS NOT NULL) ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "-- C: Adds a table comment\n",
    "COMMENT 'Clean and standardize data from the multiple-flow bronze table'\n",
    "\n",
    "-- D: Enable liquid clustering to improve performance on common filters.\n",
    "CLUSTER BY AUTO\n",
    "\n",
    "AS\n",
    "-- E: Select and clean data from the Bronze table. Uses TRY_CAST to enforce consistent types across all subsidiaries.\n",
    "SELECT\n",
    "  subsidiary_id,\n",
    "  order_id,\n",
    "  TRY_CAST(order_timestamp AS TIMESTAMP) AS order_timestamp, \n",
    "  TRY_CAST(order_date      AS DATE)      AS order_date,\n",
    "  customer_id,\n",
    "  region,\n",
    "  country,\n",
    "  city,\n",
    "  channel,\n",
    "  sku,\n",
    "  category,\n",
    "  TRY_CAST(qty          AS INT)    AS qty,\n",
    "  TRY_CAST(unit_price   AS DOUBLE) AS unit_price,\n",
    "  TRY_CAST(discount_pct AS DOUBLE) AS discount_pct,\n",
    "  TRY_CAST(total_amount AS DOUBLE) AS total_amount,\n",
    "  coupon_code\n",
    "-- F: Incrementally reads data from the bronze table that contains data from three volumes\n",
    "FROM STREAM multi_flows_1_bronze.orders_bronze_flows_demo;\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1570c2f-06ed-4d53-a180-25036849250a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code**\n",
    "- A: `CREATE OR REFRESH STREAMING TABLE` - Creates or refreshes the **multi_flows_2_silver.orders_silver_flows_demo** Silver table and applies a fixed schema to prevent schema drift across multiple flows.\n",
    "\n",
    "- B: `CONSTRAINT ... EXPECT`  Applies data quality rules that drop rows with invalid quantities, negative totals, or missing timestamps.\n",
    "\n",
    "- C: `COMMENT`  Adds descriptive metadata explaining the purpose of the Silver table.\n",
    "\n",
    "- D: `CLUSTER BY AUTO`  Enables automatic liquid clustering where Databricks intelligently chooses clustering keys to optimize your query performance. You can also specify your own clustering keys if you'd like.\n",
    "\n",
    "- E: `TRY_CAST`  Enforces consistent column types across all subsidiaries by converting raw values into standardized data types.\n",
    "\n",
    "- F: `SELECT ... FROM STREAM`  Incrementally reads, selects, and cleans records from the **multi_flows_1_bronze.orders_bronze_flows_demo** Bronze streaming table, which contains data from three separate volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f2c672a-90bd-44a5-9095-8984cf495691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E3. Run and Explore the Pipeline \n",
    "1. Select **Run pipeline** to create the Silver table.  \n",
    "   - This run executes the transformation logic and applies the data quality expectations.\n",
    "\n",
    "2. Explore the pipeline in the Lakeflow Pipeline Editor. Notice the following:\n",
    "   - Since the pipeline has already ingested the source files into Bronze, **0** rows are processed in the Bronze table.\n",
    "   - All **449** rows are processed in the Silver table.\n",
    "   - In the **Expectations** column, select `3 met` to view the data quality rules. All rows pass the expectations.\n",
    "\n",
    "\n",
    "\n",
    "#### Checkpoint\n",
    "\n",
    "![Silver SDP Checkpoint](./Includes/images/multi_flow/checkpoint_silver.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2c8c80-605a-43cc-aa88-ee647127c727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E4. Explore the Silver Streaming table\n",
    "\n",
    "1. Run the command to view the table metadata. When the results appear, notice the following:\n",
    "\n",
    "- The silver table has the exact **column data types** that were defined.\n",
    "- **Liquid clustering** is enabled:\n",
    "  - In the **# Clustering Information** section no columns are specified since Databricks hasn't optimized the keys yet (requires historical query analysis on the table to optimize the clustered columns)\n",
    "  - In the **Table Properties** row you will see cluster by auto is enabled (`clusterByAuto=true`).\n",
    "\n",
    "**NOTE:** SDP supports Liquid clustering. Liquid clustering automatically organizes data based on frequently filtered columns to improve query performance. For more information, view the [Use liquid clustering for tables](https://docs.databricks.com/aws/en/delta/clustering).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994c15c5-fd37-4a05-a634-414868a62b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED multi_flows_2_silver.orders_silver_flows_demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4938042-c5fe-41ff-833d-1ed7f8e2eb60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the code below to view the data in your **silver table**. \n",
    "\n",
    "    Confirm that the records look clean, standardized, and ready for gold-level analysis. \n",
    "    \n",
    "    Look for consistent data types, valid numeric values, and properly cast timestamps and dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01037b92-cc6c-4aba-82d0-c16f75724063",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764118966967}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM multi_flows_2_silver.orders_silver_flows_demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a8394a-a617-4362-ae17-e6da398e1f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Business Intelligence Materialized Views\n",
    "\n",
    "Materialized views in Lakeflow provide precomputed results that power fast, reliable analytics for downstream users. Unlike regular views, they automatically refresh as new data arrives, so stakeholders always see up-to-date insights without requiring on-demand computation.\n",
    "\n",
    "**NOTE:** This section assumes prior familiarity with materialized views.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "774bb7aa-2b18-43a4-b191-4e2ff9f81ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F1. Create a New SQL File in your Pipeline\n",
    "\n",
    "1. Click the kebab menu and select **New File** in the **ingest_multiple_flows** folder.\n",
    "2. Select the language as **SQL**.\n",
    "3. Name the file `gold_mvs.sql`.\n",
    "4. Keep the dataset type as `None Selected`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a99d2316-395d-4957-a21e-5ba549047ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F2. Create Simple Gold Materialized Views\n",
    "\n",
    "1. Next you will add two small simple materialized views for your consumers. These are only used to confirm that your silver data is clean and ready for analysis. Materialized views are not the focus here, so we will keep them simple.\n",
    "\n",
    "   a. The first materialized view gives a **daily summary by subsidiary**, letting you quickly check revenue, units, and order counts over time.  \n",
    "\n",
    "   b. The second view highlights **basic product performance** so you can see which categories and SKUs are selling within each subsidiary.\n",
    "\n",
    "\n",
    "2. Copy the code below into your `gold_mvs.sql` file to create both materialized views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "5a2d93ed-cba3-4542-ac4c-55c37be2ba41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "------------------------------------------\n",
    "-- a. GOLD MATERIALIZED VIEW: DAILY SUBSIDIARY SCORECARD\n",
    "-- Simple daily summary by subsidiary\n",
    "------------------------------------------\n",
    "CREATE OR REPLACE MATERIALIZED VIEW multi_flows_3_gold.mv_daily_subsidiary_scorecard_demo\n",
    "AS\n",
    "SELECT\n",
    "  order_date,\n",
    "  subsidiary_id,\n",
    "  COUNT(DISTINCT order_id)    AS order_count,   -- how many unique orders occurred\n",
    "  ROUND(SUM(total_amount),2)  AS total_revenue, -- total revenue for the day\n",
    "  SUM(qty)                    AS total_units    -- total units sold\n",
    "FROM multi_flows_2_silver.orders_silver_flows_demo\n",
    "WHERE order_date IS NOT NULL\n",
    "GROUP BY order_date, subsidiary_id;\n",
    "\n",
    "\n",
    "------------------------------------------\n",
    "-- b. GOLD MATERIALIZED VIEW: PRODUCT PERFORMANCE BY SUBSIDIARY\n",
    "-- Basic units and revenue by product and subsidiary\n",
    "------------------------------------------\n",
    "CREATE OR REPLACE MATERIALIZED VIEW multi_flows_3_gold.mv_product_performance_by_subsidiary_demo\n",
    "AS\n",
    "SELECT\n",
    "  subsidiary_id,\n",
    "  category,\n",
    "  sku,\n",
    "  SUM(qty)                   AS units_sold,  -- total units sold for each SKU\n",
    "  ROUND(SUM(total_amount),2) AS revenue      -- total revenue for each SKU\n",
    "FROM multi_flows_2_silver.orders_silver_flows_demo\n",
    "GROUP BY subsidiary_id, category, sku;\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25907e33-13c7-4b43-a1f5-8d413361a2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F3. Run and Explore the Pipeline\n",
    "\n",
    "1. Select **Run pipeline** to create the two Gold materialized views.\n",
    "\n",
    "2. Explore the pipeline in the Lakeflow Pipeline Editor. Notice the following:\n",
    "\n",
    "   - Since the pipeline has already ingested the raw data into the **Bronze** and **Silver** streaming tables, and no new files were added, there are no new records for the streaming tables to process.\n",
    "\n",
    "   - Both materialized views are created with **3** and **15** rows respectively.\n",
    "\n",
    "   - In the **Tables** window at the bottom:  \n",
    "     - Select the **Show and hide columns** button ![Show and Hide Columns](./Includes/images/show_hide_columns_icon.png)  \n",
    "     - Then show the **Incrementalization** column to see whether the materialized views were **fully recomputed** or **incrementally computed**.\n",
    "\n",
    "   - On this initial run, both materialized views show **Full recompute** on the initial creation. Later, we will see these views compute incrementally.\n",
    "\n",
    "\n",
    "\n",
    "#### Checkpoint\n",
    "\n",
    "![MVs SDP Checkpoint](./Includes/images/multi_flow/checkpoint_mvs_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a751eab4-af3c-4836-a612-6f168ea6d350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F4. Display the Materialized Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c306b2-ef15-4eac-8268-0791b1c67d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Querying the **multi_flows_3_gold.mv_daily_subsidiary_scorecard_demo** materialized view returns a daily scorecard for each subsidiary. \n",
    "\n",
    "    It summarizes order volume, total revenue, and total units so you can quickly compare performance across the three subsidiaries and order date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d51c960-fca4-4099-b310-f7d1ab9cd494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM multi_flows_3_gold.mv_daily_subsidiary_scorecard_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "171d0c35-a4f0-49e2-872f-d63245e517fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. The **multi_flows_3_gold.mv_product_performance_by_subsidiary_demo**  materialized view provides a simple product performance breakdown, showing which categories and SKUs are selling within each subsidiary. \n",
    "\n",
    "    It helps you compare units sold and revenue across product lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4d747d-5d6c-4756-9fc5-33c0ccd8645a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM multi_flows_3_gold.mv_product_performance_by_subsidiary_demo\n",
    "ORDER BY subsidiary_id, category;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "093e5637-79f2-4d03-a098-b59c3742ddb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## G. Run the Spark Declarative Pipeline with New Files\n",
    "\n",
    "Now that the pipeline is built, let's add the daily drop for the **2025-11-02** orders for each subsidiary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86c6d32-1612-4709-b580-4d640a56ee8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G1. Land a New File in Each Volume\n",
    "\n",
    "1. Run the cell below to add the next daily file (**2025-11-02**) to each subsidiary volume.  \n",
    "\n",
    "2. After the cell runs, confirm that each volume now contains two files: **2025-11-01** and **2025-11-02**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9cc1139-f964-4ec0-a5c1-2a01bd52e740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Copy a second CSV file into the bright_home_orders volume\n",
    "marketplace_share_path = f'/Volumes/{your_marketplace_share_catalog_name}/v02/subsidiary_daily_orders'\n",
    "\n",
    "copy_files(\n",
    "    copy_from = f'{marketplace_share_path}/bright_home_orders', \n",
    "    copy_to = f'/Volumes/{my_catalog}/multi_flows_1_bronze/bright_home_orders', \n",
    "    n = 2\n",
    ")\n",
    "\n",
    "## Copy a second CSV file into the lumina_sports_orders volume\n",
    "copy_files(\n",
    "    copy_from = f'{marketplace_share_path}/lumina_sports_orders', \n",
    "    copy_to = f'/Volumes/{my_catalog}/multi_flows_1_bronze/lumina_sports_orders', \n",
    "    n = 2\n",
    ")\n",
    "\n",
    "## Copy a second JSON file into the northstar_outfitters_orders volume\n",
    "copy_files(\n",
    "    copy_from = f'{marketplace_share_path}/northstar_outfitters_orders', \n",
    "    copy_to = f'/Volumes/{my_catalog}/multi_flows_1_bronze/northstar_outfitters_orders', \n",
    "    n = 2\n",
    ")\n",
    "\n",
    "\n",
    "## List files in your volumes (confirm two files exist)\n",
    "spark.sql(f'LIST \"{my_vol_path}/bright_home_orders\"').display()\n",
    "spark.sql(f'LIST \"{my_vol_path}/lumina_sports_orders\"').display()\n",
    "spark.sql(f'LIST \"{my_vol_path}/northstar_outfitters_orders\"').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9760dadd-4300-4eb3-8254-04b169fa26da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G2. Explore the New Daily Drop (2025-11-02) File in Each Volume\n",
    "1. Simply run the cell below to count the number of records in the raw daily drop file **2025-11-02** within each volume.\n",
    "\n",
    "2. Confirm that the output shows **502** total rows across all three subsidiaries for the **2025-11-02** orders drop.\n",
    "\n",
    "| Volume                      | TotalRows | FileName                    |\n",
    "|-----------------------------|-----------|------------------------------|\n",
    "| bright_home_orders          | 191       | bsh_orders_2025-11-02.csv    |\n",
    "| lumina_sports_orders        | 170       | lms_orders_2025-11-02.csv    |\n",
    "| northstar_outfitters_orders | 141       | nso_orders_2025-11-02.json   |\n",
    "| TOTAL                       | 502       |                              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738e4cbf-39e6-4996-b1aa-d769799ac894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, sum as _sum\n",
    "\n",
    "df_all = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        'bright_home_orders' AS Volume,\n",
    "        COUNT(*) AS TotalRows,\n",
    "        'bsh_orders_2025-11-02.csv' AS FileName\n",
    "    FROM read_files('{my_vol_path}/bright_home_orders/bsh_orders_2025-11-02.csv')\n",
    "\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'lumina_sports_orders' AS Volume,\n",
    "        COUNT(*) AS TotalRows,\n",
    "        'lms_orders_2025-11-02.csv' AS FileName\n",
    "    FROM read_files('{my_vol_path}/lumina_sports_orders/lms_orders_2025-11-02.csv')\n",
    "\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'northstar_outfitters_orders' AS Volume,\n",
    "        COUNT(*) AS TotalRows,\n",
    "        'nso_orders_2025-11-02.json' AS FileName\n",
    "    FROM read_files('{my_vol_path}/northstar_outfitters_orders/nso_orders_2025-11-02.json')\n",
    "\"\"\")\n",
    "\n",
    "# Build TOTAL row\n",
    "total_row = (\n",
    "    df_all\n",
    "    .agg(_sum(\"TotalRows\").alias(\"TotalRows\"))\n",
    "    .withColumn(\"Volume\", lit(\"TOTAL\"))\n",
    "    .withColumn(\"FileName\", lit(\"\"))\n",
    "    .select(\"Volume\", \"TotalRows\", \"FileName\")  # match column order\n",
    ")\n",
    "\n",
    "# Append TOTAL row to the bottom\n",
    "df_with_total = df_all.unionByName(total_row)\n",
    "\n",
    "display(df_with_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7daa56aa-52ca-421e-b5b4-59170c95d296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G3. Run and Explore the Pipeline\n",
    "\n",
    "1. Run the Spark Declarative Pipeline to **incrementally** ingest, process and aggregate the **new daily sales drop**.. Confirm it runs successfully. \n",
    "\n",
    "2. Explore the run in the Lakeflow Pipelines Editor.\n",
    "  - Confirm **502** rows were ingested into the **bronze table** from the new daily orders drop in the three volumes.\n",
    "  - Confirm all **502** were processed and passed the data quality checks in the **silver table**.\n",
    "  - Confirm the materialized views:\n",
    "    - Contain **6** and **15** rows respectively\n",
    "    - Were both incrementally refreshed (**Incremental**). \n",
    "      - **NOTE:** For more information view the [Incremental refresh for materialized views](https://docs.databricks.com/aws/en/optimizations/incremental-refresh) documentation.\n",
    "\n",
    "> **TROUBLESHOOTING:** If your pipeline does does not match the output below make sure you have landed the second file in each volume from the 'Land a New File in Each Volume' section above.\n",
    "\n",
    "#### Checkpoint\n",
    "\n",
    "![SDP Run 2 Files in All Volumes](./Includes/images/multi_flow/checkpoint_run_daily_drop_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d335d810-2731-4aac-a2e6-c609549292c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G4. Explore the Final Pipeline Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc704719-3b78-429d-bcdf-6f613f5f0c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to view the **bronze** table. \n",
    "\n",
    "    Notice that the table contains **951 rows** (The total number of rows after both runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ffa92b-d2ea-4a06-a597-49014a86a83c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764119321899}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM multi_flows_1_bronze.orders_bronze_flows_demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1df2cb58-e971-4f6b-966b-cb2d1571f70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Count the number of rows ingested by each **source_file** in the **bronze** streaming table. \n",
    "\n",
    "    Notice that we can easily examine how many rows were ingested by each source file (**daily orders drop**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f112a8d7-83ef-4835-ad5e-d3ddaf6b9e97",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763496024961}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT source_file, count(*) AS TotalRows\n",
    "FROM multi_flows_1_bronze.orders_bronze_flows_demo\n",
    "GROUP BY source_file\n",
    "ORDER BY source_file;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b2c5c00-77cc-4263-b7bf-90d79845ff38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. View the data in the **silver table**. Notice that the data is clean, adheres to our defined schema and contains our 'single source of truth'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b4bd0a-51fb-45ef-b62a-94e3746043cf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763496076495}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM multi_flows_2_silver.orders_silver_flows_demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a74c6c96-06e4-43d2-8fbf-86bc5d413948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. View the gold level materialized view **mv_daily_subsidiary_scorecard_demo**. \n",
    "\n",
    "    Notice downstream consumers can easily examine orders by date for each subsidiary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5656d503-25d0-43dd-b3d7-6885f24ead9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM multi_flows_3_gold.mv_daily_subsidiary_scorecard_demo\n",
    "ORDER BY order_date, subsidiary_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56245efd-2740-4845-b837-2ebae02a920c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. View the gold level materialized view **mv_product_performance_by_subsidiary_demo**. \n",
    "\n",
    "    Notice downstream consumers can easily examine detailed order metrics by each **subsidiary_id** and **sku**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5827c1f-7fb9-4ff5-b4d1-5ba8b8c3b6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM multi_flows_3_gold.mv_product_performance_by_subsidiary_demo\n",
    "ORDER BY subsidiary_id, category;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "984fe445-b18d-407f-93be-8112a702ed87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## H. Introduction to Adding Tags to Bronze, Silver and Gold Objects\n",
    "#### This requires the necessary permissions to add tags\n",
    "\n",
    "In this step you add semantic metadata to your tables and materialized views. Tags make it easier to organize, search and govern objects in Unity Catalog. They help downstream teams quickly understand what each object represents and how it should be used.\n",
    "\n",
    "You apply two types of tags:\n",
    "\n",
    "- **Custom demo tags**  \n",
    "  These describe the department that owns the data and the quality level in the medallion architecture.  \n",
    "  Examples:  \n",
    "  - `demo_tag_Department = 'Sales'`  \n",
    "  - `demo_tag_Quality = 'bronze' | 'silver' | 'gold'`\n",
    "\n",
    "- **System tags**  \n",
    "  These are built-in Unity Catalog tags. Here you add the `system.Certified` tag to identify trusted, production-ready objects.\n",
    "\n",
    "\n",
    "**NOTE**: For more information view the [Apply tags to Unity Catalog securable objects](https://docs.databricks.com/aws/en/database-objects/tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5b0d23-f4f3-4634-9736-0aeed80e03b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. The code below creates the following tags:\n",
    "  - Updates the **bronze table** with department and quality tags so users can see this is raw, ingested data owned by Sales.\n",
    "  - Updates the **silver table** with the same tags, then adds the `system.Certified` tag to mark it as a clean, trusted dataset.\n",
    "  - Updates both **gold materialized views** with department and quality tags, then marks each one as `system.Certified` because these are curated analytics objects intended for broad consumption.\n",
    "\n",
    "**NOTE:** [ALTER TABLE]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130ef789-3d5e-42c6-b104-7d014879f740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "------------------------------------\n",
    "-- Bronze table tags\n",
    "----------------------------------\n",
    "ALTER TABLE multi_flows_1_bronze.orders_bronze_flows_demo\n",
    "SET TAGS (\n",
    "  'demo_tag_Department' = 'Sales',\n",
    "  'demo_tag_Quality' = 'bronze'\n",
    ");\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "-- Silver table tags\n",
    "----------------------------------\n",
    "ALTER TABLE multi_flows_2_silver.orders_silver_flows_demo\n",
    "SET TAGS (\n",
    "  'demo_tag_Department' = 'Sales',\n",
    "  'demo_tag_Quality' = 'silver'\n",
    ");\n",
    "\n",
    "-- Add certified system tags\n",
    "ALTER TABLE multi_flows_2_silver.orders_silver_flows_demo\n",
    "SET TAGS ('system.Certified');\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "-- Materialized views table tags\n",
    "------------------------------------\n",
    "ALTER TABLE multi_flows_3_gold.mv_product_performance_by_subsidiary_demo\n",
    "SET TAGS (\n",
    "  'demo_tag_Department' = 'Sales',\n",
    "  'demo_tag_Quality' = 'gold'\n",
    ");\n",
    "\n",
    "ALTER TABLE multi_flows_3_gold.mv_daily_subsidiary_scorecard_demo\n",
    "SET TAGS (\n",
    "  'demo_tag_Department' = 'Sales',\n",
    "  'demo_tag_Quality' = 'gold'\n",
    ");\n",
    "\n",
    "-- Add certified system tags\n",
    "ALTER TABLE multi_flows_3_gold.mv_product_performance_by_subsidiary_demo\n",
    "SET TAGS ('system.Certified');\n",
    "\n",
    "ALTER TABLE multi_flows_3_gold.mv_daily_subsidiary_scorecard_demo\n",
    "SET TAGS ('system.Certified');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c5b530b-2364-446b-baf7-4d68390831f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. After you add tags to a table or materialized view, you can query them directly using the **information_schema.table_tags** view within the specific catalog, or the **system.information_schema** schema. \n",
    "\n",
    "    The query below queries **your-catalog.information_schema**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8efe08-d18c-40da-8b76-688a867f6612",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"tag_name\":174,\"tag_value\":160},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764176591773}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM information_schema.table_tags;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0113caaf-2a4a-4e37-a69d-32d3521ef515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. You can also view tags in **Catalog Explorer**. Follow these steps to inspect the tags on one of your objects:\n",
    "\n",
    "   a. Right click **Catalog** and select **Open in New Tab**  \n",
    "\n",
    "   b. Navigate to your catalog  \n",
    "\n",
    "   c. Open the **multi_flows_3_gold** schema  \n",
    "\n",
    "   d. Select the **mv_daily_subsidiary_scorecard_demo** materialized view  \n",
    "\n",
    "   e. In the right pane, locate the **Tags** section and confirm the three tags that were added\n",
    "\n",
    "#### Checkpoint\n",
    "![Tagging Checkpoint](./Includes/images/multi_flow/checkpoint_tagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64403546-cbfc-46fc-9c20-f041be18100f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## I. (OPTIONAL) Land Additional Files in your Volumes\n",
    "\n",
    "If you'd like to continue practicing, use the demonstration function `copy_files` to dynamically add another JSON file (file number 3) to your cloud storage location.  \n",
    "\n",
    "**NOTES:** \n",
    "- Ensure the variables you defined at the start of this lab: `your_marketplace_share_catalog_name`, `my_catalog` are still active for the function to work properly.\n",
    "- There are a total of 7 available files you can continue practicing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed0cff8-97a1-4aca-bb82-fdcf20b365b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "## Copy a second CSV file into the bright_home_orders volume\n",
    "marketplace_share_path = f'/Volumes/{your_marketplace_share_catalog_name}/v02/subsidiary_daily_orders'\n",
    "\n",
    "copy_files(\n",
    "    copy_from = f'{marketplace_share_path}/bright_home_orders', \n",
    "    copy_to = f'/Volumes/{my_catalog}/multi_flows_1_bronze/bright_home_orders', \n",
    "    n = 3 # <-- Add a third file to the volume\n",
    ")\n",
    "\n",
    "## Copy a second CSV file into the lumina_sports_orders volume\n",
    "copy_files(\n",
    "    copy_from = f'{marketplace_share_path}/lumina_sports_orders', \n",
    "    copy_to = f'/Volumes/{my_catalog}/multi_flows_1_bronze/lumina_sports_orders', \n",
    "    n = 3 # <-- Add a third file to the volume\n",
    ")\n",
    "\n",
    "## Copy a second JSON file into the northstar_outfitters_orders volume\n",
    "copy_files(\n",
    "    copy_from = f'{marketplace_share_path}/northstar_outfitters_orders', \n",
    "    copy_to = f'/Volumes/{my_catalog}/multi_flows_1_bronze/northstar_outfitters_orders', \n",
    "    n = 3 # <-- Add a third file to the volume\n",
    ")\n",
    "\n",
    "\n",
    "## List files in your volumes\n",
    "spark.sql(f'LIST \"{my_vol_path}/bright_home_orders\"').display()\n",
    "spark.sql(f'LIST \"{my_vol_path}/lumina_sports_orders\"').display()\n",
    "spark.sql(f'LIST \"{my_vol_path}/northstar_outfitters_orders\"').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5190e415-bd79-4b6e-928b-184281eb9e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## J. Clean up\n",
    "1. Feel free to delete the schemas you create in this demonstration by running cell below and confirming the delete (**Y**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6ce331-267b-4055-9b60-dadc861cd3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delete_schemas(\n",
    "    catalog = my_catalog, ## <--- Your catalog name using the variable you set earlier\n",
    "    schemas = ['multi_flows_1_bronze', 'multi_flows_2_silver', 'multi_flows_3_gold']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84420534-3c0b-4f32-a097-3cbb2fa8066e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Delete your Spark Declarative Pipeline through the **Jobs & Pipelines** UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad8c7c3e-9164-427a-8c52-41ec8cfa3848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## K. Summary and Key Takeaways \n",
    "\n",
    "- **Multi-Flow Ingestion**: Successfully ingested data from three different subsidiaries (`CSV` and `JSON` formats) into a single bronze streaming table using separate flows\n",
    "- **Schema Standardization**: Resolved format conflicts by casting all columns to STRING in bronze, then applying proper data types in silver\n",
    "- **Data Quality & Performance**: Implemented constraint-based data quality checks and enabled liquid clustering for optimized query performance\n",
    "- **Incremental Processing**: Demonstrated true incremental processing across the entire medallion architecture with automatic materialized view refresh\n",
    "\n",
    "#### Business Value Delivered\n",
    "\n",
    "The pipeline processed **951 total records** from six source files across two daily drops, creating a unified view of sales data that enables:\n",
    "- Cross-subsidiary performance analysis\n",
    "- Real-time business intelligence through auto-refreshing materialized views\n",
    "- Scalable data quality enforcement as new subsidiaries are added\n",
    "\n",
    "This architecture provides a foundation for enterprise-scale data consolidation while maintaining data lineage, quality, and performance optimization."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7961046066590728,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2 Demo - Multi Flow Data Pipeline with Liquid Clustering and Data Quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
