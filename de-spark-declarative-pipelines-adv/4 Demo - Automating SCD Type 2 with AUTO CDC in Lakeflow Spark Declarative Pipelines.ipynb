{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8584ae8d-2b9c-4313-a482-954e22302e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# INCLUDE_HEADER_TRUE\n",
    "# INCLUDE_FOOTER_TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e895aa-4566-479c-b72b-03708f76ac43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo - Automating SCD Type 2 with AUTO CDC in Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "## Overview \n",
    "\n",
    "This demonstration showcases how to implement automated Change Data Capture (CDC) for Slowly Changing Dimension (SCD) Type 2 patterns using Lakeflow Spark Declarative Pipelines. \n",
    "\n",
    "Learners will learn to build an end-to-end streaming pipeline that automatically processes customer data changes (inserts, updates, and deletes) while maintaining complete historical records. The demo uses real-world retail customer data from Databricks Marketplace and demonstrates how AUTO CDC INTO simplifies complex CDC operations that traditionally required manual coding. \n",
    "\n",
    "Learners will create a multi-layered architecture (Bronze → Silver → Gold) with automated data quality checks, incremental processing, and materialized views for both current and historical customer analytics.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this demonstration, you will be able to:\n",
    "\n",
    "- **Configure and implement AUTO CDC INTO** for automated Change Data Capture processing in Lakeflow Spark Declarative Pipelines with SCD Type 2 patterns\n",
    "\n",
    "- **Design multi-layered data architectures** using Bronze (raw ingestion), Silver (CDC processing), and Gold (analytics-ready) layers with streaming tables and materialized views\n",
    "\n",
    "- **Apply comprehensive data quality constraints** using pipeline expectations with WARN, DROP, and FAIL actions to ensure data integrity across CDC operations\n",
    "\n",
    "- **Process streaming JSON files incrementally** using Auto Loader and manage customer lifecycle events (INSERT, UPDATE, DELETE) while preserving historical versions\n",
    "\n",
    "- **Create analytics ready materialized views** that automatically maintain current customer states and track removed customers for downstream reporting and business intelligence\n",
    "\n",
    "\n",
    "### CDC Pipeline Overview\n",
    "\n",
    "In this demonstration, you'll build a Lakeflow Spark Declarative Pipeline that performs the full medallion flow from raw ingestion to curated analytics:\n",
    "\n",
    "1. **Ingest JSON source files** from cloud storage into a **Bronze raw streaming table** using Auto Loader.  \n",
    "2. **Apply data quality expectations** to the raw table to produce a **clean Bronze table** ready for downstream processing.  \n",
    "3. **Implement CDC SCD Type 2** logic with `AUTO CDC INTO` to maintain an up-to-date **Silver customer table** with full change history.  \n",
    "4. **Create Gold materialized views** for **current customers** and **deleted customers**, enabling simplified analytics and reporting.\n",
    "\n",
    "![CDC Pipeline Overview](./Includes/images/cdc_lecture/cdc_pipeline_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051b0559-e146-4889-9f37-197adbf2af43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT SERVERLESS COMPUTE VERSION 4\n",
    "\n",
    "This demonstration was developed using **Serverless v4**. \n",
    "- Selecting a serverless version: [Select an environment version](https://docs.databricks.com/aws/en/compute/serverless/dependencies#-select-an-environment-version)\n",
    "\n",
    "**NOTE:** While all purpose compute or other **Serverless** versions may work, they were not tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db1fa7ef-a39b-4895-871f-68790085d5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "Follow the cells below to set up your Workspace for the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edc85345-3bbb-48f7-a672-380a094c9413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A1. Access Marketplace Data\n",
    "\n",
    "##### NOTE: If you are running this lab in **Vocareum** as a **Partner**, the share is already installed and available as **dbacademy_retail**. Please use this as the value for the `your_marketplace_share_catalog_name` variable below.\n",
    "\n",
    "If you are running this in your own Workspace, complete the following steps to get your own copy of the Marketplace data:\n",
    "\n",
    "1. Open **Databricks Marketplace** in a new tab.  \n",
    "\n",
    "2. Search for `Simulated Retail Customer Data`.  \n",
    "\n",
    "3. Select the tile titled **Simulated Retail Customer Data (Databricks provided)**.  \n",
    "\n",
    "4. Click **Get instant access**.  \n",
    "\n",
    "5. **Enter a unique catalog name** for your share to avoid receiving a duplicate catalog error in shared Workspaces. For example: `dbacademy_retail_yourname`.  \n",
    "\n",
    "6. Review and accept the terms, then click **Get instant access** to complete the setup.\n",
    "\n",
    "7. Update the variable `your_marketplace_share_catalog_name` in cell below to point to your shared catalog from Marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb65b4f-8916-4e20-86ad-e7134bcc2895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Update the variable below to reference your marketplace catalog name\n",
    "\n",
    "## NOTE: If you are using Vocareum use the value 'dbacademy_retail' catalog below\n",
    "your_marketplace_share_catalog_name = 'YOUR_MARKETPLACE_SHARE_CATALOG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f8bb15b-4fd1-4c6f-a8cd-94b17304734a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A2. Configure Your Catalog and Schema\n",
    "\n",
    "1. In this step, you must **specify a catalog that you own or have write access to**.\n",
    "    - If you already have a catalog, update the code below to use its name.\n",
    "    - If you do not yet have one, create a catalog first, then return and update the code.\n",
    "\n",
    "    Replace the value for the `my_catalog` variable with your catalog name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8602689-62dc-481f-9c8b-3148c9938084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_catalog = 'YOUR_CATALOG'   ## <---- Simply replace with your catalog name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feaf75c5-4aec-4f61-adaa-4b94a734d5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cells below to setup your demonstration environment. The setup will:\n",
    "\n",
    "    - Check the variables from above were created\n",
    "    - Create three schemas in your specified catalog:  \n",
    "      - **sdp_cdc_1_bronze**\n",
    "      - **sdp_cdc_2_silver**\n",
    "      - **sdp_cdc_3_gold**  \n",
    "    - Creates a volume named **your-catalog.sdp_cdc_1_bronze.customer_source_files** and adds a single JSON file.\n",
    "    - Checks your specified compute\n",
    "\n",
    "    This ensures that all schemas, tables and objects are created in your catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5db9fab-7072-4c16-8502-2082027271e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-auto-cdc-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13b39aa-6bf1-415f-a06b-402ba21ff0d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_vol_path = auto_cdc_demo_setup(\n",
    "    my_catalog = my_catalog, \n",
    "    marketplace_catalog = your_marketplace_share_catalog_name, \n",
    "    schema = 'sdp_cdc',\n",
    "    source_volume = 'customer_source_files',\n",
    "    reset_volume = False ## <-- Set to True to delete all files in your volumes to start fresh if you've already complete the demo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a03e643-46fa-4aab-88d1-6fbc97a29da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Confirm the variable `my_vol_path` created in the classroom setup points to **your-catalog.sdp_cdc_1_bronze.customer_source_files** volume path.\n",
    "\n",
    "    **Example:** `/Volumes/your-catalog/sdp_cdc_1_bronze/customer_source_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e82674ef-8c3c-48ca-a283-1442e864c2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(my_vol_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68c12a98-a592-4587-afc9-0d4dd030fbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore the Customer Data Source File(s) in Your Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d2d0972-4329-4fb1-809d-d698b1dd1854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to programmatically view the files in your `/Volumes/your-catalog/sdp_cdc_1_bronze/customer_source_files/` volume **path**.\n",
    "  \n",
    "    Confirm you only see one file for customers (**00.json**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4ade90-5225-44d7-a850-f392209f61cf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":481},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762537222884}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'LIST \"{my_vol_path}\"').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33560c61-0ae5-464f-857a-1d3468999d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the query below to explore the customers **00.json** file located in your **customer_source_files** volume within **your-catalog.sdp_cdc_1_bronze** schema. \n",
    "\n",
    "    Note the following:\n",
    "\n",
    "      a. The file contains **939 customers** (remember this number).\n",
    "\n",
    "      b. It includes general customer information such as **email**, **name**, and **address**.\n",
    "\n",
    "      c. The **timestamp** column specifies the logical order (sequence) of customer events in the source data as a UNIX timestamp.\n",
    "\n",
    "      d. The **operation** column indicates whether the entry is for a new customer, a deletion, or an update.\n",
    "\n",
    "      - **NOTE:** Since this is the first JSON file, all rows will be considered new customers.\n",
    "\n",
    "  **NOTE:** The `my_vol_path` SQL variable was created for you in the classroom setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28369fe-a0da-4dc1-861d-7ae3add572b9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762692797750}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  my_vol_path || '/00.json',  -- my_vol_path is the path to your source JSON files in the volume your-catalog.sdp_cdc_1_bronze.customer_source_files\n",
    "  format => \"JSON\"\n",
    ")\n",
    "ORDER BY operation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f561bd2-fd39-476f-af1c-8488752dfa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question\n",
    "How can we ingest new raw JSON source files with customer updates into our pipeline to update a **silver table** when inserts, updates, or deletes occur, while also maintaining historical records (SCD Type 2)?\n",
    "\n",
    "### Answer\n",
    "Use `AUTO CDC INTO` with Spark Declarative Pipelines!\n",
    "\n",
    "View the Databricks documentation [Processing a change data feed: Keep only the latest data vs. keep historical versions of data](https://docs.databricks.com/aws/en/ldp/what-is-change-data-capture#processing-a-change-data-feed-keep-only-the-latest-data-vs-keep-historical-versions-of-data) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b37b014-6b9c-43f1-b5e4-10c50f13421f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Building the CDC Spark Declarative Pipeline with `AUTO CDC INTO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "560bcacb-de3a-4a7a-a770-df90e1785921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Enable the Lakeflow Pipelines Editor\n",
    "\n",
    "Complete the following steps to confirm or enable the **Lakeflow Pipelines Editor**:\n",
    "\n",
    "1. In the top-right corner of the workspace, select your **account icon** ![Account Icon](./Includes/images/account_icon.png) (*Your letter will differ*).  \n",
    "\n",
    "2. Right-click **Settings** and choose **Open link in new tab**.  \n",
    "3. In the left sidebar, select **Developer** under **User**.  \n",
    "4. In the **Experimental features** section, locate **Lakeflow Pipelines Editor** and toggle it **on**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab30406-ac4e-4045-a5d4-016e3647bd21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Create a Lakeflow Spark Declarative Pipeline using the Lakeflow Pipelines Editor\n",
    "Complete the following steps to create your Spark Declarative Pipeline:\n",
    "\n",
    "1. In the main navigation pane, right-click **Jobs & Pipelines** and select **Open link in New Tab**.  \n",
    "\n",
    "2. In the new tab, select **Create → ETL Pipeline**.  \n",
    "\n",
    "   **NOTE:** If prompted to **Try the new Lakeflow Pipelines Editor**, choose **Enable Lakeflow Pipelines Editor**. This appears only if you did not complete the previous step.  \n",
    "\n",
    "3. At the top, complete the following:\n",
    "   - Name your pipeline `demo_auto_cdc_pipeline_yourname`\n",
    "   - Select your default **catalog** and **schema**:  \n",
    "        - **Catalog:** The catalog you specified for this notebook  \n",
    "        - **Schema:** **sdp_cdc_1_bronze**  \n",
    "\n",
    "4. Select **Start with an empty file**. In the pop-up window, specify the following:  \n",
    "   - For **Location where the pipeline folder will be created** - Specify the folder this notebook resides in.\n",
    "   - Select **SQL**\n",
    "   - Select **Create**\n",
    "\n",
    "5. Rename the **transformations** folder to `my_pipeline`.\n",
    "\n",
    "6. Rename the `my_transformations.sql` file to `cdc_pipeline.sql`.\n",
    "\n",
    "7. Leave the **Lakeflow Pipelines Editor** page open.\n",
    "\n",
    "#### Checkpoint\n",
    "![Create SDP Checkpoint](./Includes/images/cdc_lecture/cdc_create_initial_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d32f268-1dca-4ba1-9eba-45183aa9d93c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Configure Pipeline Parameter\n",
    "\n",
    "1. Run the cell below to get the path to your raw data source JSON files in your **customer_source_files** volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8c089a-ebcd-4d78-8724-3b62006e14c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(my_vol_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc3519b-d259-4566-8e45-90b0bc0287ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Copy the path above and add it as a configuration parameter in your **Spark Declarative Pipeline**. This will reference your **sdp_cdc_1_bronze.customer_source_files** volume that contains your raw JSON file(s) that you explored earlier:\n",
    "\n",
    "   a. Select **Settings** in your pipeline tab.\n",
    "\n",
    "   b. Under **Configuration** select **Add configuration**.\n",
    "\n",
    "   c. For **Key** add `source`\n",
    "\n",
    "   d. For **Value** add the `path to your volume` from above.\n",
    "\n",
    "   e. Select **Save**.\n",
    "\n",
    "   **NOTE:** For more information on configuration parameters, check out the Databricks documentation [Use parameters with Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/ldp/parameters)\n",
    "\n",
    "#### Checkpoint (your path will vary)\n",
    "<img src=\"./Includes/images/cdc_lecture/cdc_config_parameter_value.png\" alt=\"Config Parameter Checkpoint\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9dcf7bb-853c-4171-9c1d-9ee6f0eb908e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C4. Create Bronze Layer with Auto Loader for Incremental Ingestion\n",
    "\n",
    "1. In this step, you'll define the **Bronze raw** layer of your **Spark Declarative Pipeline** to create the table: **sdp_cdc_1_bronze.customers_bronze_raw_demo**.  \n",
    "\n",
    "   This layer will incrementally ingest raw **JSON** files from cloud storage into a **streaming table** using **Auto Loader**.  \n",
    "   \n",
    "   Copy the SQL code below and paste it into your `cdc_pipeline.sql` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "e1ca7a6b-d4ba-4a69-b204-f6c3e2260636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "------------------------------------------------------\n",
    "-- STEP 1: JSON -> Bronze Ingestion\n",
    "------------------------------------------------------\n",
    "-- Ingest the JSON files from cloud storage into a streaming table using Auto Loader\n",
    "CREATE OR REFRESH STREAMING TABLE sdp_cdc_1_bronze.customers_bronze_raw_demo\n",
    "  COMMENT \"Raw data from customers CDC feed\"\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() processing_time,  -- Obtain the ingestion processing time for the rows\n",
    "  _metadata.file_name as source_file    -- Obtain the file name of the record\n",
    "FROM STREAM read_files(\n",
    "  \"${source}\",  -- References your SDP parameter that points to your demonstration volume containing your JSON data source files\n",
    "  format => \"json\");\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8c79ee3-f3c6-4777-8692-a702df771525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code:**\n",
    "\n",
    "- `CREATE OR REFRESH STREAMING TABLE` - Creates a managed streaming table that automatically updates as new data arrives.  \n",
    "- `COMMENT` - Describes the purpose of the table for easier discovery and documentation.  \n",
    "- `current_timestamp()` - Captures the ingestion timestamp for data lineage and auditing.  \n",
    "- `_metadata.file_name` - Adds the source file name for traceability and debugging.  \n",
    "- `FROM STREAM read_files()` - Uses Auto Loader to continuously read new JSON files from the specified `${source}` path you added in the previous step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e275ec-ca40-4ad6-8e7e-00720a3e8c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Select **Dry Run** to verify that your pipeline and SQL code are configured correctly.  \n",
    "   \n",
    "   This will validate your settings without actually running the full pipeline.\n",
    "\n",
    "#### Checkpoint\n",
    "> **TROUBLESHOOTING:** If the dry run did not work, make sure you configure your `source` pipeline parameter correctly in the **Configure Pipeline Parameter** step above.\n",
    "\n",
    "<img src=\"./Includes/images/cdc_lecture/cdc_dry_run_ingest.png\" alt=\"JSON Ingest\" width=\"1100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "994068e6-a445-4cab-b9ad-49e3e741f39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C5. Apply Data Quality Rules to the Bronze Table\n",
    "\n",
    "With the raw JSON CDC data now streaming into **sdp_cdc_1_bronze.customers_bronze_raw_demo**, we'll add another **bronze** quality layer before moving to **Silver**.  \n",
    "\n",
    "You'll create an intermediate table **sdp_cdc_1_bronze.customers_bronze_clean** and apply **data quality constraints** to filter and flag invalid records while preserving expected `NULL` values for `DELETE` operations.  \n",
    "\n",
    "In this step, you'll:  \n",
    "- Apply **multiple constraints** to check key fields.  \n",
    "- Use **WARN**, **DROP**, and **FAIL** actions to define how violations are handled.  \n",
    "- Add **conditional logic** and **regex validation** inside constraints.  \n",
    "\n",
    "For details, see [Manage data quality with pipeline expectations](https://docs.databricks.com/aws/en/ldp/expectations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36063070-d506-47c2-941f-aefb6afbedc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C5.1 - About the Data Source\n",
    "\n",
    "   - The CDC feed contains `INSERT`, `UPDATE`, and `DELETE` operations for customer records. \n",
    "\n",
    "   - `INSERT` and `UPDATE` events include valid values for all key columns.  \n",
    "\n",
    "   - `DELETE` events have `NULL` values for all non-key fields such as **name**, **email**, **address**, **city**, and **state**.  \n",
    "\n",
    "   **Example of a Dropped Record:**\n",
    "\n",
    "   | **address** | **city** | **customer_id** | **email** | **name** | **operation** | **state** |\n",
    "   |--------------|-----------|------------------|-------------|------------|----------------|-------------|\n",
    "   | `NULL` | `NULL` | `23617` | `NULL` | `NULL` | `DELETE` | `NULL` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5053deb4-8386-4485-837d-96405c560224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C5.2 - Create the Clean Bronze Table Using Pipeline Data Quality Expectations\n",
    "\n",
    "1. Now that we have a general understanding of the data, let's define the following **data quality constraints** that enforce those rules in a secondary Bronze table named **sdp_cdc_1_bronze.customers_bronze_clean_demo** before the data moves to the Silver stage.\n",
    "\n",
    "Below is a table of the data quality constraints:\n",
    "\n",
    "  | **Comment ID** | **Constraint** | **Rule / Expectation** | **Violation Action** | **Purpose** |\n",
    "  |:------:|----------------|------------------------|----------------------|--------------|\n",
    "  | **A** | **`valid_id`** | **customer_id** `IS NOT NULL` | **FAIL** | Fails the transaction if a record has a missing **customer_id**. |\n",
    "  | **B** | **`valid_operation`** | **operation** `IS NOT NULL` | **DROP ROW** | Drops any record missing an **operation** type. |\n",
    "  | **C** | **`valid_name`** | **name** `IS NOT NULL OR operation = \"DELETE\"` | **WARN** (default) | Flags missing **name** values unless it's a **DELETE** operation. |\n",
    "  | **D** | **`valid_address`** | All address fields (**address**, **city**, **state**, **zip_code**) must be `non-null` unless it's a **DELETE** | **WARN** (default) | Ensures complete address information for **INSERT** and **UPDATE** events. |\n",
    "  | **E** | **`valid_email`** | **email** must match a valid format unless it's a **DELETE** | **DROP ROW** | Uses regex to validate the **email** format and drops invalid records. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "793d5989-4a58-4d53-99b0-22c873028ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Copy the SQL code below and paste it into your `cdc_pipeline.sql` file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "9e3a8b33-7a3f-4f8b-9a8e-d0a6fc366dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    " ------------------------------------------------------\n",
    "-- STEP 2: Bronze Raw -> Bronze Clean\n",
    "------------------------------------------------------\n",
    "CREATE STREAMING TABLE sdp_cdc_1_bronze.customers_bronze_clean_demo\n",
    "  (\n",
    "\n",
    "    -- A. Require a valid customer_id, fail the transaction if missing\n",
    "    CONSTRAINT valid_id EXPECT (customer_id IS NOT NULL) \n",
    "      ON VIOLATION FAIL UPDATE,\n",
    "\n",
    "    -- B. Require a valid operation, drop any record with NULL operation\n",
    "    CONSTRAINT valid_operation EXPECT (operation IS NOT NULL) \n",
    "      ON VIOLATION DROP ROW,\n",
    "\n",
    "    -- C. Require name to be present unless the operation is DELETE\n",
    "    CONSTRAINT valid_name EXPECT (name IS NOT NULL OR operation = \"DELETE\"),\n",
    "\n",
    "    -- D. Require full address fields unless operation is DELETE\n",
    "    CONSTRAINT valid_address EXPECT (\n",
    "      (address IS NOT NULL \n",
    "       AND city IS NOT NULL \n",
    "       AND state IS NOT NULL \n",
    "       AND zip_code IS NOT NULL)\n",
    "       OR operation = \"DELETE\"),\n",
    "\n",
    "    -- E. Require valid email format (regex), skip check for DELETE, drop invalid rows\n",
    "    CONSTRAINT valid_email EXPECT (\n",
    "      rlike(email, '^([a-zA-Z0-9_\\\\-\\\\.]+)@([a-zA-Z0-9_\\\\-\\\\.]+)\\\\.([a-zA-Z]{2,5})$') \n",
    "      OR operation = \"DELETE\") \n",
    "      ON VIOLATION DROP ROW\n",
    "  )\n",
    "  COMMENT \"Clean raw bronze data and apply quality constraints\"\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  CAST(from_unixtime(timestamp) AS timestamp) AS timestamp_datetime -- Convert unix timestamp\n",
    "FROM STREAM sdp_cdc_1_bronze.customers_bronze_raw_demo;\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17a479ea-e201-4653-8ce4-c8dd3a786825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code:**\n",
    "\n",
    "  - This query creates the **sdp_cdc_1_bronze.customers_bronze_clean_demo** table from the **sdp_cdc_1_bronze.customers_bronze_raw_demo** table.  \n",
    "\n",
    "  - It applies the **data quality constraints** defined above to ensure only valid CDC records progress to the next stage before reaching Silver.\n",
    "\n",
    "  - Also **Converts UNIX timestamp to readable timestamp** using `CAST(from_unixtime(timestamp) AS timestamp)` as `timestamp_datetime`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc4b20b9-682a-4c33-acdc-3490cb55114e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Select **Dry Run** to verify that your pipeline and SQL code are configured correctly.  \n",
    "   \n",
    "   This will validate your settings without actually running the full pipeline.\n",
    "\n",
    "#### Checkpoint\n",
    "<img src=\"./Includes/images/cdc_lecture/cdc_dry_run_bronze_clean.png\" alt=\"Bronze Clean\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cad96f6-9c90-4b82-8cd8-f47ffabafc03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C6.Implement `AUTO CDC INTO` for the Silver Table\n",
    "\n",
    "Now that the **Bronze** layer (**sdp_cdc_1_bronze.customers_bronze_raw_demo -> sdp_cdc_1_bronze.customers_bronze_clean_demo**) data is clean, we'll create a **Silver** table that automatically applies inserts, updates, and deletes as new changes arrive.  \n",
    "\n",
    "#### PROBLEM \n",
    "Traditionally, handling Change Data Capture (CDC) manually can be complex and error-prone.  \n",
    "\n",
    "#### SOLUTION \n",
    "`AUTO CDC INTO` in **Spark Declarative Pipelines** simplifies this process by automatically managing inserts, updates, and deletes in streaming data, reducing boilerplate code and improving reliability.\n",
    "\n",
    "`AUTO CDC INTO` provides the following guarantees and requirements:\n",
    "\n",
    "- Performs incremental and streaming ingestion of CDC data  \n",
    "- Lets you define one or more primary key fields for identifying records  \n",
    "- Assumes rows contain inserts and updates by default  \n",
    "- **Optionally** applies deletes when defined  \n",
    "- Orders late-arriving records using a **sequencing key** \n",
    "- Allows excluding columns with the **`EXCEPT`** keyword  \n",
    "- Defaults to **SCD Type 1**, though we'll use **SCD Type 2** in this demonstration  \n",
    "\n",
    "\n",
    "We'll complete this in two steps:\n",
    "\n",
    "1. **Create an empty Silver target table** to store historical customer data using the SCD Type 2 pattern.  \n",
    "\n",
    "2. **Use `AUTO CDC INTO`** to automatically apply inserts, updates, and deletes as new changes arrive.\n",
    "\n",
    "\n",
    "**NOTE:** For more details, see the official documentation: [**The AUTO CDC APIs: Simplify change data capture with Lakeflow Declarative Pipelines**](https://docs.databricks.com/aws/en/ldp/cdc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd14df4a-2cec-4e01-9df6-6ffb7beb0e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C6.1 - Create an Empty Silver Target Table\n",
    "Before we can use AUTO CDC INTO, we need to create the target table that will store our customer data with historical tracking.\n",
    "\n",
    "1. The SQL command below creates or refreshes a **streaming Silver table** named **sdp_cdc_2_silver.customers_silver_scd2_demo**.  \n",
    "\n",
    "   This table will continuously receive incremental updates from the **customers_bronze_clean_demo** table as new change data arrives.  \n",
    "\n",
    "   Copy and paste the code below into your `cdc_pipeline.sql` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "718f191d-5ecd-4c18-98ef-24af6957b07d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "---------------------------------------------------------------------------------------\n",
    "-- STEP 3: Processing CDC Data with AUTO CDC INTO\n",
    "---------------------------------------------------------------------------------------\n",
    "\n",
    "-- a. Create the streaming target table if it's not already created\n",
    "CREATE OR REFRESH STREAMING TABLE sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "  COMMENT 'SCD Type 2 Historical Customer Data';\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d42f0802-deb9-4152-a46d-d9ed019f9b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code**\n",
    "\n",
    "- `CREATE OR REFRESH STREAMING TABLE` - Creates the table **sdp_cdc_2_silver.customers_silver_scd2_demo** if it doesn't exist, or refreshes it if it does, ensuring it's ready for streaming updates.\n",
    "\n",
    "- `COMMENT` - Adds a description to document the table's purpose, identifying it as an **SCD Type 2** table used to track historical customer changes over time.\n",
    "\n",
    "Once the Silver target table exists, we can automatically propagate changes from the cleaned Bronze table using Lakeflow Declarative Pipelines with the `AUTO CDC INTO` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca47cb7-ecd1-4b8a-a348-476d9178198e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C6.2 - Perform SCD Type 2 with `AUTO CDC INTO`\n",
    "\n",
    "\n",
    "1. Before we run `AUTO CDC INTO`, let's take a closer look at the key columns in our cleaned Bronze table (**sdp_cdc_1_bronze.customers_bronze_clean_demo**). \n",
    "\n",
    "    Understanding these columns helps explain how **Spark Declarative Pipelines** detects and applies changes across `INSERT`, `UPDATE`, and `DELETE` operations.\n",
    "\n",
    "    Below is a simplified preview of the data. We've included only the columns that are important for **Change Data Capture (CDC)**.\n",
    "\n",
    "| **customer_id** (*primary key*) | **name** | **email** | **operation** (*type of change*) | **timestamp_datetime** (*sequence column*) | **source_file** | ... |\n",
    "|------------------|-----------|------------|----------------|------------------------|------------------|-----|\n",
    "| 23056 | Brent Chavez | nelsonjoy@example.com | `NEW` | 2021-09-23T17:26:21.000+00:00 | 00.json | ... |\n",
    "| 23057 | James Cruz | perkinsdeborah@example.net | `UPDATE` | 2021-09-23T18:21:45.000+00:00 | 00.json | ... |\n",
    "| 23058 | Jennifer Christensen | jmccullough@example.net | `DELETE` | 2021-09-23T00:19:44.000+00:00 | 00.json | ... |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "**Key Columns**\n",
    "\n",
    "- **customer_id**- Serves as the **primary key** used to identify each unique customer record. CDC logic depends on this column to determine which row should be `updated` or `deleted` in the target table.\n",
    "\n",
    "- **operation** - Indicates the type of change from the source system (`NEW`, `UPDATE`, or `DELETE`).  \n",
    "  - `DELETE` operations remove the corresponding record from the target table.  \n",
    "  - `NEW` and `UPDATE` operations are automatically processed and merged by `AUTO CDC` using the **customer_id** primary key column.\n",
    "\n",
    "- **timestamp_datetime** - Defines the **order of operations**. This ensures `updates` are applied in the correct sequence when multiple changes occur for the same record.\n",
    "\n",
    "- **source_file** - Helps trace which file each record came from, useful for debugging and validation during pipeline runs.\n",
    "\n",
    "Next, we'll use these columns to perform `AUTO CDC INTO`, allowing Spark Declarative Pipelines **to automatically manage incremental updates between Bronze and Silver**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eef3952-bb52-4ab3-afb7-5aff24f88618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. The `AUTO CDC INTO` command defines a **Lakeflow Declarative Pipeline flow** that applies **SCD Type 2** Change Data Capture logic to keep the **Silver** table continuously up to date.  \n",
    "\n",
    "   It automatically manages how `INSERTS`, `UPDATES`, and `DELETES` are applied between the following:  \n",
    "\n",
    "   - `Source:` **sdp_cdc_1_bronze.customers_bronze_clean_demo** - The streaming Bronze table containing cleaned CDC records.  \n",
    "\n",
    "   - `Target:` **sdp_cdc_2_silver.customers_silver_scd2_demo** - The Silver table that stores the current and historical customer data.\n",
    "\n",
    "\n",
    "    Copy and paste the code below into your `cdc_pipeline.sql` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "a64c1d57-3d50-47e4-a32a-14767bca24a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "-- b. Perform SCD Type 2 into the silver table\n",
    "CREATE FLOW customers_scd_type_2_flow AS \n",
    "AUTO CDC INTO sdp_cdc_2_silver.customers_silver_scd2_demo  -- Target: Where processed records are stored\n",
    "FROM STREAM sdp_cdc_1_bronze.customers_bronze_clean_demo   -- Source: Clean CDC records from Bronze layer\n",
    "  KEYS (customer_id)                                       -- Primary key: Used to match records for updates/deletes\n",
    "  APPLY AS DELETE WHEN operation = \"DELETE\"                -- Delete logic: Remove records marked as DELETE\n",
    "  SEQUENCE BY timestamp_datetime                           -- Ordering: Ensures changes are applied in correct sequence\n",
    "  COLUMNS * EXCEPT (timestamp, _rescued_data, operation)   -- Column selection: Include all except metadata fields\n",
    "  STORED AS SCD TYPE 2;                                    -- SCD Type 2: Maintains historical versions with __START_AT and __END_AT\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b2fb64-e17f-4873-8cec-6f506ec2f7b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Review the code**\n",
    "\n",
    "- `CREATE FLOW customers_scd_type_2_flow AS` - Creates a named flow (`customers_scd_type_2_flow`) that defines how CDC changes will be processed.  \n",
    "- `AUTO CDC INTO sdp_cdc_2_silver.customers_silver_scd2_demo` - Applies the CDC logic to the **target** Silver table.  \n",
    "- `FROM STREAM sdp_cdc_1_bronze.customers_bronze_clean_demo` - Reads the streaming **source data** that includes new inserts, updates, and deletes.  \n",
    "- `KEYS (customer_id)` - Identifies the unique customer record by its primary key.  \n",
    "- `APPLY AS DELETE WHEN operation = \"DELETE\"` - Ensures records with a delete operation are removed from the target.  \n",
    "- `SEQUENCE BY timestamp_datetime` - Orders incoming records so late-arriving data is processed correctly.  \n",
    "- `COLUMNS * EXCEPT (timestamp, _rescued_data, operation)` - Selects all columns from the source except metadata or system fields.  \n",
    "- `STORED AS SCD TYPE 2` - Specifies the **Slowly Changing Dimension Type 2** method, which updates records in place, keeping historical versions.\n",
    "\n",
    "- **NOTE:** For more information view the Databricks documentation [AUTO CDC INTO (Lakeflow Declarative Pipelines)](https://docs.databricks.com/aws/en/ldp/developer/ldp-sql-ref-apply-changes-into)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30925b8d-51dc-4fb8-951a-4046944cf690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Select **Dry Run** to verify that your pipeline and SQL code are configured correctly.  \n",
    "   \n",
    "   This will validate your settings without actually running the full pipeline.\n",
    "\n",
    "#### Checkpoint\n",
    "<img src=\"./Includes/images/cdc_lecture/cdc_dry_run_auto_cdc_run1.png\" alt=\"AUTO CDC\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20d43f72-84a2-4909-8ead-4ac2b5e5bcbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Pipeline Execution and Analysis (Run with 1 JSON File)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a66d74-06db-465b-b79a-9d38eda4a394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. Run the Spark Declarative Pipeline\n",
    "\n",
    "1. In the **Lakeflow Pipelines Editor**, run your **Spark Declarative Pipeline**. The first run may take a few minutes.  \n",
    "\n",
    "2. Once the pipeline finishes, verify it looks similar to the example below.\n",
    "\n",
    "#### Checkpoint\n",
    "\n",
    "<img src=\"./Includes/images/cdc_lecture/cdc_01_pipeline-run.png\" alt=\"Pipeline Run 1\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b9ce3b-6a96-4d55-9cf6-1866e70ac0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Quick Review: AUTO CDC INTO (SCD Type 2 Behavior)\n",
    "\n",
    "\n",
    "<img src=\"./Includes/images/cdc_lecture/02-scd-type-2-01-review-slide.png\" alt=\"SCD Type 2 Review\" width=\"1200\">\n",
    "\n",
    "Recall `AUTO CDC INTO` in Spark Declarative Pipelines automatically tracks historical changes without manual merge logic. With **SCD Type 2** it creates two new columns:\n",
    "\n",
    "- **__START_AT** - Timestamp when the record becomes active. \n",
    "\n",
    "- **__END_AT** - Timestamp when the record is closed (after an update or delete).  \n",
    "\n",
    "When a row changes, the old version gets an **__END_AT** timestamp, and a new version starts with a fresh **__START_AT**. \n",
    "\n",
    "This keeps a full SCD Type 2 history automatically.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9bc1e35-77f4-4848-ab5e-45c61a510ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D3. Explore the Spark Declarative Pipeline Results\n",
    "\n",
    "After the pipeline completes, review the outputs to confirm data flow and CDC behavior on the first run with **one** JSON file.\n",
    "\n",
    "1. In the pipeline graph:\n",
    "   - **939** rows were streamed through all layers, passing Bronze data quality checks and upserting into **sdp_cdc_2_silver.customers_silver_scd2_demo**.\n",
    "\n",
    "2. In the **Lakeflow Pipeline Editor** table window:\n",
    "   - The **Upserted records** column in **sdp_cdc_2_silver.customers_silver_scd2_demo** confirms all **939** rows were inserted as new customers.\n",
    "\n",
    "3. In the **Tables** view, select **customers_silver_scd2_demo**:\n",
    "   - Preview the table to inspect the data.\n",
    "   - Scroll right to see the **__START_AT** and **__END_AT** columns  were automatically added by the `AUTO CDC INTO` SCD Type 2 process indicating active, inactive and removed data (right now all rows are active - **__END_AT** = `null`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34689857-1568-4922-b138-9c37665aee4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D4. Explore the Pipeline CDC Silver Streaming Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd045600-9360-43d5-9a5c-2d25663f1993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to confirm that `current_catalog()` references your catalog and that your settings are still active (they might have been cleared).  \n",
    "\n",
    "    If they were cleared, reset your default catalog using: `USE CATALOG your_catalog_name;`\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5106004b-43cf-4fb2-9f38-9a071f9fbd28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog() AS `Your Current Catalog`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a30fb8-05a6-4620-a0e0-bb7ef7b4dcb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the query below to view the **sdp_cdc_2_silver.customers_silver_scd2_demo** CDC streaming table (SCD Type 2).  \n",
    "\n",
    "Observe the following:\n",
    "\n",
    "- The table contains all **939 rows** from the **00.json** file, as all customers are new.  \n",
    "\n",
    "- Scroll to the right to see that `AUTO CDC INTO (SCD Type 2)` added two columns:  \n",
    "  - **__START_AT**: Timestamp showing when the current version of each record became active. This matches the sequence column **timestamp_datetime**.  \n",
    "  - **__END_AT**: \n",
    "    - Timestamp showing when the record became inactive due to a **DELETE** or **UPDATE**. \n",
    "    - Filtering where **__END_AT** is `NULL` returns all active customers.  \n",
    "\n",
    "Since this is the first ingestion from **00.json**, all records are active and **__END_AT** contains only `NULL` values.\n",
    "\n",
    "**NOTES:**  \n",
    "- These columns implement **Slowly Changing Dimension (SCD) Type 2** tracking.  \n",
    "- `AUTO CDC INTO` automatically manages **__START_AT** and **__END_AT** to record the valid time range of each version.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8b13b0-62af-48d0-a41b-ea82104612a6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762791855930}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT customer_id, email, timestamp_datetime, processing_time, source_file, __START_AT, __END_AT\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6ca64e1-d7eb-49db-96b6-a98abfa1300b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell below to query the CDC table for **all current customers** by filtering where **__END_AT** is `NULL`. \n",
    "\n",
    "   Recall a `NULL` value in the **__END_AT** column indicates all active (current) records.\n",
    "\n",
    "   Review the results and confirm that on the first run all **939** records are active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647fc835-e308-4938-a0c3-e5754d6a7fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT customer_id, email, timestamp_datetime, processing_time, source_file, __START_AT, __END_AT\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "WHERE __END_AT IS NULL;    -- Find all current customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea7866b7-41ec-4e8b-b9e7-cfa9236e9b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. To find all **deleted customer records** in an SCD Type 2 table created with `AUTO CDC INTO`, you need to locate the **most recent version of each record** and check if the **__END_AT** `IS NOT NULL` (contains a value).  \n",
    "\n",
    "    - **NOTE:** Recall a non-null **__END_AT** value indicates that the record has been marked as deleted.  \n",
    "\n",
    "    The example query below demonstrates one way to find these records. Run the cell and confirm that **0** results are returned, since no deletions have occurred yet in the **sdp_cdc_2_silver.customers_silver_scd2_demo** CDC table. \n",
    "\n",
    "\n",
    "**Code details**\n",
    "- This identifies **removed customers** by grouping all versions, selecting each customer's latest record, and keeping only those marked as deleted, showing their final details before removal.\n",
    "\n",
    "    - **Use MAX_BY to find the latest record** - The function `MAX_BY(value, __START_AT)` returns the value from the row with the most recent `__START_AT` timestamp.  \n",
    "        - `MAX_BY(name, __START_AT)` - most recent name  \n",
    "        - `MAX_BY(address, __START_AT)` - most recent address  \n",
    "        - `MAX_BY(__END_AT, __START_AT)` - most recent deletion marker from the most recent record\n",
    "        - etc.\n",
    "\n",
    "    - `GROUP BY customer_id` - Collects all historical versions of each customer into a single group so we can analyze changes over time.\n",
    "\n",
    "    - `HAVING MAX_BY(__END_AT, __START_AT) IS NOT NULL` - Condition keeps only customers whose latest record has a value for `__END_AT`, meaning they have been removed.\n",
    "\n",
    "    - [max_by aggregate function](https://docs.databricks.com/aws/en/sql/language-manual/functions/max_by)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe66781-d001-494e-8616-c3d1f57a550e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Currently this query will return 0 results since no customers were marked as deleted\n",
    "SELECT\n",
    "  customer_id,\n",
    "  MAX_BY(name, __START_AT)      AS name,\n",
    "  MAX_BY(address, __START_AT)   AS address,\n",
    "  MAX_BY(city, __START_AT) AS city,\n",
    "  MAX_BY(state, __START_AT)   AS state,\n",
    "  MAX_BY(zip_code, __START_AT)   AS zip_code,\n",
    "  MAX_BY(__START_AT, __START_AT) AS __START_AT,\n",
    "  MAX_BY(__END_AT, __START_AT)  AS __END_AT\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "GROUP BY customer_id\n",
    "HAVING MAX_BY(__END_AT, __START_AT) IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "994b109f-1d1c-43aa-9739-fcefd1b8ec8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Create Gold Materialized Views\n",
    "\n",
    "Rather than having users manually query the CDC table to **find current or deleted customers**, you can create **Gold materialized views** that surface this information automatically as an object.  \n",
    "\n",
    "These views simplify access for business users and can be added directly to your **Lakeflow Declarative Pipeline** for continuous updates (incremental if possible).\n",
    "\n",
    "In this step we'll create two views:\n",
    "\n",
    "   - a. **current_customers_gold_demo** - Returns only the most recent, active customer records (where `__END_AT IS NULL`).  \n",
    "\n",
    "   - b. **removed_customers_gold_demo** - Creates a **Gold Materialized View** that lists all customers who have been **removed (deleted)** from the SCD Type 2 table.  (It uses `MAX_BY` on multiple columns like `name`, `address`, `city`, `state`, and `zip_code` to return the most recent record per customer where the latest `__END_AT` value is not `NULL`.)\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "- Materialized views in **Lakeflow Declarative Pipelines** provide an automatically updated layer for reporting and dashboards. They make it easy to query only the **current customer state** or identify **removed customers** without scanning the entire SCD history.\n",
    "\n",
    "- When possible, **materialized views** are incrementally updated. For more information, see the [Incremental refresh for materialized views](https://docs.databricks.com/aws/en/optimizations/incremental-refresh).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8135a34d-ffd2-4c52-adab-3fbd44de667d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E1. Create the Current Customers Materialized View\n",
    "\n",
    "1. Copy and paste the code below into your `cdc_pipeline.sql` file to create your gold materialized views. Explore the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "14b69a8a-2459-4d2b-b26f-22f85652b4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "---------------------------------------------------------------------------------------\n",
    "-- STEP 4: Create Materialized View for Current (Active) Customers\n",
    "---------------------------------------------------------------------------------------\n",
    "\n",
    "-- a. Create Gold Materialized View for Current Customers\n",
    "CREATE OR REFRESH MATERIALIZED VIEW sdp_cdc_3_gold.current_customers_gold_demo\n",
    "COMMENT \"Current updated list of active customers\"\n",
    "AS \n",
    "SELECT \n",
    "  * EXCEPT (processing_time),\n",
    "  current_timestamp() updated_at\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "WHERE `__END_AT` IS NULL;      -- Filter for only rows that contain a null value for __END_AT, which indicates the current version of the record\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72ab9407-d3f5-4bc5-af73-17accf3c47c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E2. Create the Deleted Customers Materialized View\n",
    "\n",
    "1. Copy and paste the code below into your `cdc_pipeline.sql` file to create your gold materialized views. Explore the code.\n",
    "\n",
    "- **NOTE:** Because this uses `GROUP BY` with aggregates instead of window functions, Lakeflow can update the view incrementally as new CDC data arrives instead of recomputing everything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "f780868e-7c52-4020-a098-c708c64c2bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<button onclick=\"copyBlock()\">Copy to clipboard</button>\n",
    "\n",
    "<pre id=\"copy-block\" style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; border:1px solid #e5e7eb; border-radius:10px; background:#f8fafc; padding:14px 16px; font-size:0.85rem; line-height:1.35; white-space:pre;\">\n",
    "<code>\n",
    "<!-------------------ADD SOLUTION CODE BELOW------------------->\n",
    "-- b. Create Gold Materialized View that lists all customers who have been removed (deleted) from the SCD Type 2 table.\n",
    "CREATE OR REFRESH MATERIALIZED VIEW sdp_cdc_3_gold.removed_customers_gold_demo AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  MAX_BY(name, __START_AT)      AS name,\n",
    "  MAX_BY(address, __START_AT)   AS address,\n",
    "  MAX_BY(city, __START_AT) AS city,\n",
    "  MAX_BY(state, __START_AT)   AS state,\n",
    "  MAX_BY(zip_code, __START_AT)   AS zip_code,\n",
    "  MAX_BY(__START_AT, __START_AT) AS __START_AT,\n",
    "  MAX_BY(__END_AT, __START_AT)  AS __END_AT\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "GROUP BY customer_id\n",
    "HAVING MAX_BY(__END_AT, __START_AT) IS NOT NULL;  -- Find the latest records __END_AT value and only return if not null\n",
    "<!-------------------END SOLUTION CODE------------------->\n",
    "</code></pre>\n",
    "\n",
    "<script>\n",
    "function copyBlock() {\n",
    "  const el = document.getElementById(\"copy-block\");\n",
    "  if (!el) return;\n",
    "\n",
    "  const text = el.innerText;\n",
    "\n",
    "  // Preferred modern API\n",
    "  if (navigator.clipboard && navigator.clipboard.writeText) {\n",
    "    navigator.clipboard.writeText(text)\n",
    "      .then(() => alert(\"Copied to clipboard\"))\n",
    "      .catch(err => {\n",
    "        console.error(\"Clipboard write failed:\", err);\n",
    "        fallbackCopy(text);\n",
    "      });\n",
    "  } else {\n",
    "    fallbackCopy(text);\n",
    "  }\n",
    "}\n",
    "\n",
    "function fallbackCopy(text) {\n",
    "  const textarea = document.createElement(\"textarea\");\n",
    "  textarea.value = text;\n",
    "  textarea.style.position = \"fixed\";\n",
    "  textarea.style.left = \"-9999px\";\n",
    "  document.body.appendChild(textarea);\n",
    "  textarea.select();\n",
    "  try {\n",
    "    document.execCommand(\"copy\");\n",
    "    alert(\"Copied to clipboard\");\n",
    "  } catch (err) {\n",
    "    console.error(\"Fallback copy failed:\", err);\n",
    "    alert(\"Could not copy to clipboard. Please copy manually.\");\n",
    "  } finally {\n",
    "    document.body.removeChild(textarea);\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc26534a-4a79-4ec1-a06c-ba3a2a701d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E3. Run the Final Spark Declarative Pipeline with the Materialized Views\n",
    "\n",
    "1. **REQUIRED:** Since you're developing the pipeline, run it using **Run pipeline with full table refresh** to remove all checkpoints, truncates the tables, and runs the pipeline again.\n",
    "    - (select the **Run Pipeline** dropdown arrow → **Run pipeline with full table refresh**).  \n",
    "\n",
    "   This action will:  \n",
    "   - Remove all existing checkpoints  \n",
    "   - Drop previously created tables  \n",
    "   - Restart the pipeline from scratch \n",
    "\n",
    "2. While it's running, update the bottom table view to show the **Incrementalization** column.  \n",
    "   - **NOTE:** The **Incrementalization** column shows whether each materialized view was processed as a **Full recompute** or updated **incrementally**: [Incremental refresh for materialized views](https://docs.databricks.com/aws/en/optimizations/incremental-refresh).  \n",
    "   \n",
    "   <img src=\"./Includes/images/cdc_lecture/incremental_mvs_column2.png\" alt=\"MV Incremental Column\" width=\"1000\"> \n",
    "<br></br>\n",
    "3. After the run completes, verify the pipeline results match the example below.\n",
    "\n",
    "#### Checkpoint\n",
    "\n",
    "> **TROUBLESHOOTING:** If you see zero rows processed in your streaming tables, you likely ran a normal pipeline run instead of **Run pipeline with full table refresh**. A standard run will not reprocess data that was already ingested and processed.\n",
    "<img src=\"./Includes/images/cdc_lecture/cdc_02_pipeline-run-with-mvs-one-file.png\" alt=\"Pipeline Run 2 - MVs\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "130ac32f-9bfe-4b4d-af37-0ed5fe2ae9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E4. Explore the Spark Declarative Pipeline Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67cdff64-aa13-4291-9b44-5406593c4c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After the pipeline completes, review the outputs to confirm data flow and CDC behavior.\n",
    "\n",
    "1. In the pipeline graph:\n",
    "   - **939** rows were streamed through all streaming tables, passing Bronze data quality checks and upserting into **sdp_cdc_2_silver.customers_silver_scd2_demo**.\n",
    "   - **sdp_cdc_3_gold.current_customers_gold_demo** shows **939** active customers, as no updates or deletes have occurred yet and all customers are active.\n",
    "   - **sdp_cdc_3_gold.removed_customers_gold_demo** shows **0** rows since no customers have been removed.\n",
    "\n",
    "2. In the **Lakeflow Pipeline Editor** table window:\n",
    "   - The **Upserted records** column in **sdp_cdc_2_silver.customers_silver_scd2_demo** confirms all **939** rows were inserted as new customers.\n",
    "   - Both Gold materialized views display `Full recompute` in the **Incrementalization** column because this was their first computation.\n",
    "\n",
    "3. In the **Tables** view, select **customers_silver_scd2_demo**:\n",
    "   - Preview the table to inspect the data.\n",
    "   - Scroll right to see the **__START_AT** and **__END_AT** columns automatically added by the `AUTO CDC INTO` SCD Type 2 process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb5c32db-07e5-40f3-9ffb-c185967348c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Incremental Data Processing\n",
    "The final pipeline is built, let's add another file to cloud storage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc38dbfc-4687-4620-90cd-3554ab8ec327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F1. Land Additional Data\n",
    "1. Before loading additional data, query the **sdp_cdc_2_silver.customers_silver_scd2_demo** streaming table for customers *23225* and *23617*.  \n",
    "\n",
    "   Review the following details:\n",
    "\n",
    "   - **customer_id = 23225**  \n",
    "     - **Address:** `76814 Jacqueline Mountains Suite 815`  \n",
    "     - **State:** `TX`  \n",
    "     - **__END_AT:** `null`, indicating this is the current active record for the customer.\n",
    "\n",
    "   - **customer_id = 23617**  \n",
    "     - This record currently exists in the table and is active (**__END_AT** is `null`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c912f398-5e5d-407a-87d7-f4e217b1c153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "WHERE customer_id IN (23225, 23617);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fd2412-1ce6-467d-a0e3-039955b6b7a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell below to land a new JSON file (**01.json**) **in** your **customer_source_files** volume to simulate new files being added to your cloud storage location.\n",
    "\n",
    "    Confirm that your **customer_source_files** volume contains two JSON files (**00.json and 01.json**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f24a61f-88e1-4ee1-9887-29c2eb1e9dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Copy a second JSON file into the user's volume\n",
    "copy_files(\n",
    "    copy_from = f'/Volumes/{your_marketplace_share_catalog_name}/v01/retail-pipeline/customers/stream_json', \n",
    "    copy_to = f'/Volumes/{my_catalog}/sdp_cdc_1_bronze/customer_source_files', \n",
    "    n = 2\n",
    ")\n",
    "\n",
    "## List files in your volume (confirm two files exist)\n",
    "spark.sql(f'LIST \"{my_vol_path}\"').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bd5fcf2-7f59-48b3-9192-81e136ca6a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the cell below to view the raw data in **01.json** before ingesting it into your pipeline.  \n",
    "\n",
    "   Key points to notice:\n",
    "\n",
    "   - The **01.json** file contains **23** rows.  \n",
    "   \n",
    "   - The **operation** column includes **UPDATE**, **DELETE**, and **NEW** values:\n",
    "     - 12 customers marked as **UPDATE**  \n",
    "     - 1 customer marked as **DELETE**  \n",
    "     - 10 customers marked as **NEW**  \n",
    "\n",
    "   - For **customer_id = 23225** (Sandy Adams):  \n",
    "     - Original address (from **00.json**): `76814 Jacqueline Mountains Suite 815`, `TX`  \n",
    "     - Updated address (in this file): `512 John Stravenue Suite 239`, `TN`  \n",
    "\n",
    "   - For **customer_id = 23617**:  \n",
    "     - The **operation** is **DELETE**.  \n",
    "     - All other columns contain `NULL` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da99763-b269-4ab5-b799-c1c767b679aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  my_vol_path || '/01.json',\n",
    "  format => \"JSON\"\n",
    ")\n",
    "ORDER BY customer_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b235dd-e3be-44f3-9180-e188c95eb86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F2. Process Updates and Deletes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a58eea59-dc02-4da3-b731-7a03764a188a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Return to your **Spark Declarative Pipeline** and select **Run pipeline** to incrementally process the new JSON file (**01.json**) and apply SCD Type 2 logic to your Silver table.  \n",
    "\n",
    "2. Once complete, verify the results match the example below.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "<img src=\"./Includes/images/cdc_lecture/cdc_run_01json.png\" alt=\"Pipeline Run 2\" width=\"1200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083fb23a-3bd8-4678-9f84-e50c2c76e67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. After running the pipeline, review the **Pipeline graph**:\n",
    "\n",
    "   - **23 rows** were incrementally ingested from the new **01.json** file into both **customers_bronze_raw_demo** and **customers_bronze_clean_demo** , passing all data quality checks. \n",
    "\n",
    "   - In the **customers_silver_scd2_demo** streaming table (SCD Type 2), **35 rows** were processed and **upserted**.  \n",
    "     - Breakdown of the **35** upserts from the **01.json** file:  \n",
    "        - **12 updates** - each update adds a new version and marks the old one inactive (**24 total upserts**)  \n",
    "        - **1 delete** - sets a value in the **__END_AT** column  \n",
    "        - **10 new** customer inserts \n",
    "\n",
    "     **Total changes:** 24 + 1 + 10 = **35**\n",
    "\n",
    "   - The **current_customers_gold_demo** materialized view contains **948** active customers.\n",
    "\n",
    "   - The **removed_customers_gold_demo** materialized view contains **1** deleted customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e06fc87-45e0-4f3b-8eb2-98e7652302c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## G. Pipeline Object Analysis and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "758c6e46-389e-4010-b413-0e65ba515b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G1. Query the CDC Silver Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b2acc44-1958-4598-9725-d7d29f4224dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Query the data in the **sdp_cdc_2_silver.customers_silver_scd2_demo** streaming table with SCD Type 2 and observe the following:\n",
    "\n",
    "   a. The table contains **961 rows**\n",
    "\n",
    "      - **initial 939 customers**  \n",
    "      - \\+ **12 updates** to existing customers \n",
    "      - \\+ **10 new customers**\n",
    "\n",
    "   b. Scroll to the right and locate the **__END_AT** column. Then scroll down to rows **82** and **83**. Notice there are two rows for customer **22668**, the original record and the updated record.\n",
    "\n",
    "**NOTE:** For demonstration purposes, many of the metadata columns were retained in the silver streaming table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "925482ae-5535-40db-a307-f6ebc7fb2016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT customer_id, address, name, __START_AT, __END_AT\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "ORDER BY customer_id, __END_AT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c2e61b3-4ad8-4d44-8a9f-a691ca95ba14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the query on the **sdp_cdc_2_silver.customers_silver_scd2_demo** streaming table for all rows where **__END_AT** `IS NOT NULL` to view all rows where those customers rows are now inactive.\n",
    "\n",
    "Notice the following:\n",
    "  - **13 rows** are returned (**12 UPDATES** + **1 DELETE**)\n",
    "  - The **__END_AT** column indicates the date and time that the row was either updated or marked as removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128f2afc-7376-4995-ae95-d01240aa2e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT customer_id, address, name, __START_AT, __END_AT\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "WHERE __END_AT IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77bdce18-a641-4a86-8103-60e71350e66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Query the **sdp_cdc_2_silver.customers_silver_scd2_demo** table for the **customer_id** *23225* (one of the customers that was updated). \n",
    "\n",
    "Notice the following:\n",
    "\n",
    "- There are **two records** for that customer in the table.\n",
    "- The original record from the **00.json** file now has a value in the **__END_AT** column, indicating that it is now inactive.\n",
    "- The new record from the **01.json** file is now the active row and contains a `null` value in the **__END_AT** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b3f3db-6f88-4bbb-8bc3-2609407eb4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT customer_id, address, name, state, source_file, __START_AT, __END_AT\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "WHERE customer_id = 23225;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5d992e7-5cd2-4ba3-a962-617ae262d71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. In the **01.json** file, recall **customer_id** *23617* was marked as deleted. \n",
    "\n",
    "    Let's query the **sdp_cdc_2_silver.customers_silver_scd2_demo** table for that customer and view the results. \n",
    "\n",
    "    Notice that when a customer is marked as deleted, the **__END_AT** column contains the value of when that customer was deleted and became inactive, but the customer records STILL EXISTS in the **sdp_cdc_2_silver.customers_silver_scd2_demo** with SCD Type 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899b4eff-b3cb-403e-b910-5b6fe8a3c77c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT customer_id, address, name, __START_AT, __END_AT\n",
    "FROM sdp_cdc_2_silver.customers_silver_scd2_demo\n",
    "WHERE customer_id = 23617"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e065eab-c815-48df-98d6-413dac85c8ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G2. Query the Current Customers Materialized View\n",
    "\n",
    "1. To view your organization's most up-to-date customer data, you can query the materialized view **sdp_cdc_3_gold.current_customers_gold_demo**. \n",
    "\n",
    "    Remember, the query to create the materialized view filters for all **__END_AT** values that are `null` (active rows).\n",
    "\n",
    "    Run the cell and view the results. Notice the following:\n",
    "   - The current updated list of customers contains **948 rows**:\n",
    "     - **939** from the initial file (**00.json**)\n",
    "     - **+10** new customers from the update file (**01.json**)\n",
    "     - **-1** deleted customer from the update file (**01.json**)\n",
    "     - The table also contains the updated records from the **01.json** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45410c6d-92f4-46e0-9793-d5709b3271cf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765300078618}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT customer_id, address, name, __START_AT, __END_AT, source_file\n",
    "FROM sdp_cdc_3_gold.current_customers_gold_demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a2078c7-1940-4d15-bdd0-f335500f27ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G3. Query the Removed Customers Materialized View\n",
    "1. To view your organization's deleted customers, you can query the materialized view **sdp_cdc_3_gold.removed_customers_gold_demo**. \n",
    "\n",
    "    Run the cell and view the results. Notice that there has been **1** customer marked as deleted (**customer_id** = *23617*) in the CDC silver table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715d313d-0a7f-4da0-b21a-43733ada7d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM sdp_cdc_3_gold.removed_customers_gold_demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb277248-2f91-4775-b7b1-3e92d3ef54f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## H. (Optional) Continue Incrementally Processing New Data\n",
    "\n",
    "If you'd like to continue practicing, use the demonstration function `copy_files` to dynamically add another JSON file to your cloud storage location.  \n",
    "\n",
    "**NOTE:** Ensure the variables you defined at the start of this lab: `your_marketplace_share_catalog_name`, `my_catalog` are still active for the function to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df971d72-df82-4239-80b3-c3934fe1f726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "## Copy a third JSON file into the user's volume\n",
    "copy_files(\n",
    "    copy_from = f'/Volumes/{your_marketplace_share_catalog_name}/v01/retail-pipeline/customers/stream_json', \n",
    "    copy_to = f'/Volumes/{my_catalog}/sdp_cdc_1_bronze/customer_source_files', \n",
    "    n = 3   # <-- This value determines how many files should be in the volume. This will add the third json file 02.json to the volume\n",
    ")\n",
    "\n",
    "# Display the total files in the volume\n",
    "spark.sql(f'LIST \"{my_vol_path}\"').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02d39f93-f544-4da0-ae21-12097bb6dc54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## I. Lab Clean Up\n",
    "1. Feel free to delete the schemas you create in this demonstration by running cell below and confirming the delete (**Y**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffbbf34-be27-4d6e-afa6-425c98f18707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delete_schemas(\n",
    "    catalog = my_catalog, ## <--- Your catalog name using the variable you set earlier\n",
    "    schemas = ['sdp_cdc_1_bronze','sdp_cdc_2_silver','sdp_cdc_3_gold']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1485399-a42a-4939-b13d-e1f404651fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Delete your Spark Declarative Pipeline through the **Jobs & Pipelines** UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b56cac01-0721-48ea-9e9c-468e19b78d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## J. Summary and Key Takeaways\n",
    "\n",
    "You have successfully implemented an end-to-end **AUTO CDC for SCD Type 2** pipeline using **Lakeflow Spark Declarative Pipelines**. This demonstration showcased how modern data engineering can dramatically simplify complex Change Data Capture operations that traditionally required extensive manual coding.\n",
    "\n",
    "**Simplified CDC Implementation:**\n",
    "- Replaced complex manual `MERGE` operations with declarative `AUTO CDC INTO` syntax\n",
    "- Automatic handling of late-arriving data using sequence columns\n",
    "- Built-in SCD Type 2 logic without manual custom coding\n",
    "\n",
    "**Robust Data Quality:**\n",
    "- Multi-layered validation with conditional constraints for DELETE operations\n",
    "- Flexible violation handling (WARN, DROP, FAIL) based on business requirements\n",
    "- Comprehensive email validation using regex patterns\n",
    "\n",
    "**Incremental Processing Efficiency:**\n",
    "- Auto Loader for continuous file ingestion from cloud storage\n",
    "- Incremental materialized view updates where possible\n",
    "- Streaming architecture that processes only new changes"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7961046066591115,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4 Demo - Automating SCD Type 2 with AUTO CDC in Lakeflow Spark Declarative Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
